{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9653af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Dataset scan + build data.yaml for pose (robust scan)\n",
    "from pathlib import Path\n",
    "import os, yaml\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "\n",
    "root = Path('./dataset')\n",
    "assert root.exists(), \"Dataset folder './dataset' not found. Please scp/rsync it first.\"\n",
    "\n",
    "# Find YOLO label .txt anywhere under labels/ (supports both layouts)\n",
    "label_txts = []\n",
    "label_txts += glob(str(root / 'labels' / '**' / '*.txt'), recursive=True)          # type-first: dataset/labels/train/*.txt\n",
    "label_txts += glob(str(root / '*' / 'labels' / '**' / '*.txt'), recursive=True)    # split-first: dataset/train/labels/*.txt\n",
    "label_txts = sorted(set(label_txts))\n",
    "print(f\"Found {len(label_txts)} label files under 'labels' folders\")\n",
    "\n",
    "if len(label_txts) == 0:\n",
    "    # Last resort: search any .txt and warn\n",
    "    label_txts = glob(str(root / '**' / '*.txt'), recursive=True)\n",
    "    print(f\"Fallback: found {len(label_txts)} .txt files total\")\n",
    "    assert len(label_txts)>0, \"No label .txt files found anywhere in './dataset'. Expected YOLO-pose labels under .../labels/...\"\n",
    "\n",
    "# Robustly infer keypoints K by majority across many labels\n",
    "kp_counter = Counter()\n",
    "for i, fp in enumerate(label_txts[:2000]):  # sample up to 2000 files\n",
    "    try:\n",
    "        with open(fp, 'r') as f:\n",
    "            for j, ln in enumerate(f):\n",
    "                ln = ln.strip()\n",
    "                if not ln:\n",
    "                    continue\n",
    "                parts = ln.split()\n",
    "                # prefer full pose labels: cls cx cy w h + 3*K\n",
    "                if len(parts) >= 8 and (len(parts) - 5) % 3 == 0:\n",
    "                    Kcand = (len(parts) - 5) // 3\n",
    "                    kp_counter[Kcand] += 1\n",
    "                # stop early per file after a couple of lines\n",
    "                if j >= 2:\n",
    "                    break\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "if not kp_counter:\n",
    "    # Fallback: detect K from kpt-only (cls x1 y1 x2 y2 ...)\n",
    "    kpt_only_counter = Counter()\n",
    "    for i, fp in enumerate(label_txts[:2000]):\n",
    "        try:\n",
    "            with open(fp, 'r') as f:\n",
    "                for j, ln in enumerate(f):\n",
    "                    ln = ln.strip()\n",
    "                    if not ln:\n",
    "                        continue\n",
    "                    parts = ln.split()\n",
    "                    if len(parts) >= 3 and (len(parts) - 1) % 2 == 0:\n",
    "                        Kcand = (len(parts) - 1) // 2\n",
    "                        kpt_only_counter[Kcand] += 1\n",
    "                    if j >= 2:\n",
    "                        break\n",
    "        except Exception:\n",
    "            continue\n",
    "    assert kpt_only_counter, 'Could not infer keypoints K from labels.'\n",
    "    K = max(kpt_only_counter, key=kpt_only_counter.get)\n",
    "else:\n",
    "    K = max(kp_counter, key=kp_counter.get)\n",
    "\n",
    "print(f\"Inferred keypoints K (majority) = {K}\")\n",
    "\n",
    "# Heuristics to resolve images dirs for each split\n",
    "def choose_images_dir(split: str):\n",
    "    candidates = [\n",
    "        root / 'images' / split,       # type-first\n",
    "        root / split / 'images',       # split-first\n",
    "    ]\n",
    "    # derive from labels/<split>\n",
    "    split_label_dirs = []\n",
    "    for p in label_txts:\n",
    "        p_norm = p.replace('\\\\', '/').lower()\n",
    "        if f\"/labels/{split}/\" in p_norm:\n",
    "            split_label_dirs.append(Path(p).parent)\n",
    "    if split_label_dirs:\n",
    "        ldir = split_label_dirs[0]\n",
    "        candidates += [\n",
    "            ldir.parent / 'images',                  # .../train/images\n",
    "            ldir.parent.parent / 'images' / ldir.name # .../images/train\n",
    "        ]\n",
    "    for c in candidates:\n",
    "        if c.exists():\n",
    "            return str(c)\n",
    "    return None\n",
    "\n",
    "train_path = choose_images_dir('train')\n",
    "val_path = choose_images_dir('val')\n",
    "test_path = choose_images_dir('test')\n",
    "print('Images dirs (raw):', {'train': train_path, 'val': val_path, 'test': test_path})\n",
    "\n",
    "# Normalize to be RELATIVE to dataset root if possible, else absolute\n",
    "root_abs = root.resolve()\n",
    "\n",
    "def to_rel_or_abs(p):\n",
    "    if not p:\n",
    "        return None\n",
    "    p_abs = Path(p)\n",
    "    try:\n",
    "        p_abs = p_abs.resolve()\n",
    "    except Exception:\n",
    "        p_abs = (root / p).resolve()\n",
    "    try:\n",
    "        return str(p_abs.relative_to(root_abs))\n",
    "    except ValueError:\n",
    "        return str(p_abs)\n",
    "\n",
    "train_rel = to_rel_or_abs(train_path)\n",
    "val_rel = to_rel_or_abs(val_path)\n",
    "test_rel = to_rel_or_abs(test_path)\n",
    "print('Images dirs (normalized):', {'train': train_rel, 'val': val_rel, 'test': test_rel})\n",
    "\n",
    "assert train_rel and val_rel, (\n",
    "    \"Could not locate images/train and/or images/val. Ensure YOLO structure like:\\n\"\n",
    "    \"- dataset/images/train & dataset/labels/train\\n\"\n",
    "    \"- dataset/train/images & dataset/train/labels\\n\"\n",
    ")\n",
    "\n",
    "# Build YAML for pose\n",
    "names = ['plate']\n",
    "skeleton = [[0,1],[1,2],[2,3],[3,0]] if K==4 else []\n",
    "flip_idx = list(range(K))\n",
    "\n",
    "data = {\n",
    "    'path': str(root_abs),\n",
    "    'train': train_rel,\n",
    "    'val': val_rel,\n",
    "    'test': test_rel,\n",
    "    'names': names,\n",
    "    'kpt_shape': [K, 3],\n",
    "    'skeleton': skeleton,\n",
    "    'flip_idx': flip_idx,\n",
    "}\n",
    "\n",
    "yaml_path = root/'data.yaml'\n",
    "with open(yaml_path, 'w') as f:\n",
    "    yaml.safe_dump(data, f, sort_keys=False)\n",
    "\n",
    "print('Wrote', yaml_path)\n",
    "print(yaml.safe_dump(data, sort_keys=False))\n",
    "\n",
    "# Clear label caches so Ultralytics rebuilds with the corrected kpt_shape\n",
    "for cache_name in ['train.cache', 'val.cache']:\n",
    "    cp = root / 'labels' / cache_name\n",
    "    if cp.exists():\n",
    "        try:\n",
    "            os.remove(cp)\n",
    "            print('Removed cache:', cp)\n",
    "        except Exception as e:\n",
    "            print('Failed to remove cache', cp, e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df84a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.b) Merge extra datasets (datasets2 + dataset3/LP_detection) into ./dataset\n",
    "from pathlib import Path\n",
    "import shutil, os, glob\n",
    "\n",
    "root_main = Path('./dataset')\n",
    "root_ds2 = Path('./datasets2')\n",
    "root_ds3 = Path('./dataset3/LP_detection')  # use LP_detection split in dataset3\n",
    "\n",
    "assert root_main.exists(), \"./dataset must exist (target)\"\n",
    "print('Target dataset:', root_main.resolve())\n",
    "\n",
    "# Ensure subfolders\n",
    "for p in [root_main/'images'/ 'train', root_main/'images'/'val', root_main/'labels'/'train', root_main/'labels'/'val']:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Helper to copy tree contents (images + labels) preserving names\n",
    "IMG_EXTS = {'.jpg','.jpeg','.png','.bmp','.tif','.tiff','.webp'}\n",
    "\n",
    "def copy_split(src_images: Path, src_labels: Path, dst_images: Path, dst_labels: Path):\n",
    "    if src_images and src_images.exists():\n",
    "        files = [p for p in src_images.rglob('*') if p.suffix.lower() in IMG_EXTS]\n",
    "        print(f'Copying {len(files)} images from', src_images)\n",
    "        for fp in files:\n",
    "            rel = fp.name\n",
    "            shutil.copy2(fp, dst_images/rel)\n",
    "    if src_labels and src_labels.exists():\n",
    "        files = list(src_labels.rglob('*.txt'))\n",
    "        print(f'Copying {len(files)} labels from', src_labels)\n",
    "        for fp in files:\n",
    "            rel = fp.name\n",
    "            shutil.copy2(fp, dst_labels/rel)\n",
    "\n",
    "# Merge from datasets2 (assumes YOLO layout)\n",
    "if root_ds2.exists():\n",
    "    copy_split(root_ds2/'images'/'train', root_ds2/'labels'/'train', root_main/'images'/'train', root_main/'labels'/'train')\n",
    "    copy_split(root_ds2/'images'/'val',   root_ds2/'labels'/'val',   root_main/'images'/'val',   root_main/'labels'/'val')\n",
    "else:\n",
    "    print('datasets2 not found, skip')\n",
    "\n",
    "# Merge from dataset3/LP_detection (assumes YOLO layout)\n",
    "if root_ds3.exists():\n",
    "    copy_split(root_ds3/'images'/'train', root_ds3/'labels'/'train', root_main/'images'/'train', root_main/'labels'/'train')\n",
    "    copy_split(root_ds3/'images'/'val',   root_ds3/'labels'/'val',   root_main/'images'/'val',   root_main/'labels'/'val')\n",
    "else:\n",
    "    print('dataset3/LP_detection not found, skip')\n",
    "\n",
    "# Clear caches so Ultralytics reindexes\n",
    "for cache_name in ['train.cache', 'val.cache']:\n",
    "    cp = root_main / 'labels' / cache_name\n",
    "    if cp.exists():\n",
    "        try:\n",
    "            os.remove(cp)\n",
    "            print('Removed cache:', cp)\n",
    "        except Exception as e:\n",
    "            print('Failed to remove cache', cp, e)\n",
    "\n",
    "print('Merge done. Now re-run the data.yaml build cell above to re-assert paths if needed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c2501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Train\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "import torch, os, logging\n",
    "\n",
    "# Show full per-epoch logs (progress bar + metrics)\n",
    "os.environ['TQDM_DISABLE'] = '0'  # enable tqdm progress bars\n",
    "os.environ.setdefault('ULTRALYTICS_HUB', '0')\n",
    "os.environ.setdefault('WANDB_DISABLED', 'true')\n",
    "try:\n",
    "    from ultralytics.utils import LOGGER\n",
    "    LOGGER.setLevel(logging.INFO)  # show epoch summaries and metrics\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Use local data.yaml and local best.pt only\n",
    "yaml_path = Path('./dataset/data.yaml')\n",
    "assert yaml_path.exists(), \"dataset/data.yaml not found. Run the dataset cell first.\"\n",
    "\n",
    "ckpt = str(Path('best.pt').resolve())\n",
    "assert os.path.exists(ckpt), \"best.pt not found at project root. Place your checkpoint at ./best.pt\"\n",
    "\n",
    "print(f'Training from: {ckpt}')\n",
    "model = YOLO(ckpt)\n",
    "\n",
    "# Training args with visible epoch metrics\n",
    "train_args = dict(\n",
    "    data=str(yaml_path),\n",
    "    imgsz=640,        # bigger image size helps pose accuracy\n",
    "    epochs=200,       # train up to 200 epochs\n",
    "    patience=30,      # early stop if no val improvement for 30 epochs\n",
    "    batch=4,          # adjust if you hit OOM\n",
    "    device=0 if torch.cuda.is_available() else 'cpu',\n",
    "    workers=4,        # avoid multiprocessing issues in notebooks/Python 3.13\n",
    "    project='runs',\n",
    "    name='pose_plate',\n",
    "    exist_ok=True,\n",
    "    pretrained=False,  # avoid triggering any default weight downloads\n",
    "    cache='ram',\n",
    "    plots=True,       # save results.png and curves\n",
    "    save_period=0,    # checkpoint every N epochs (0=only best/last)\n",
    "    seed=42,\n",
    "    amp=False,        # disable AMP to avoid online AMP check (no yolo11n.pt)\n",
    "    verbose=True,     # print full per-epoch metrics\n",
    ")\n",
    "\n",
    "results = model.train(**train_args)\n",
    "print('Training done. Best:', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f539c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Validate & quick inference\n",
    "from glob import glob\n",
    "import os, yaml\n",
    "import cv2, numpy as np\n",
    "\n",
    "# Strict offline mode: allow local assets but block any remote downloads\n",
    "os.environ.setdefault('ULTRALYTICS_HUB', '0')\n",
    "os.environ.setdefault('WANDB_DISABLED', 'true')\n",
    "try:\n",
    "    from pathlib import Path as _P\n",
    "    from ultralytics.utils import downloads as _ud\n",
    "    from ultralytics.utils import checks as _uc\n",
    "    _ud.is_online = lambda: False  # type: ignore\n",
    "\n",
    "    def _local_only(asset, *args, **kwargs):\n",
    "        # Return the local path if it exists; otherwise block\n",
    "        try:\n",
    "            p = _P(asset)\n",
    "        except TypeError:\n",
    "            try:\n",
    "                p = _P(asset[0])\n",
    "            except Exception:\n",
    "                p = None\n",
    "        if p is not None and p.exists():\n",
    "            return str(p)\n",
    "        raise RuntimeError(f\"Ultralytics offline: asset not found locally: {asset}\")\n",
    "\n",
    "    for _name in (\"safe_download\", \"attempt_download\", \"attempt_download_asset\", \"get_github_assets\"):\n",
    "        if hasattr(_ud, _name):\n",
    "            setattr(_ud, _name, _local_only)\n",
    "    if hasattr(_uc, \"check_requirements\"):\n",
    "        _uc.check_requirements = lambda *a, **k: None  # type: ignore\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "\n",
    "# Force local checkpoint only (absolute path)\n",
    "ckpt = str(Path('best.pt').resolve())\n",
    "assert os.path.exists(ckpt), \"Local checkpoint './best.pt' not found. Place your trained weights as best.pt at project root.\"\n",
    "print('Loading weights:', ckpt)\n",
    "model = YOLO(ckpt)\n",
    "print('Model task:', getattr(model, 'task', None))\n",
    "\n",
    "# read data.yaml to find val images path\n",
    "with open('./dataset/data.yaml', 'r') as f:\n",
    "    data_cfg = yaml.safe_load(f)\n",
    "val_path = data_cfg.get('val')\n",
    "base_path = data_cfg.get('path', '.')\n",
    "val_dir = os.path.join(base_path, val_path) if val_path else './dataset/val/images'\n",
    "\n",
    "# pick a few images\n",
    "val_imgs = []\n",
    "for pat in ['*.jpg', '*.jpeg', '*.png', '*.*']:\n",
    "    val_imgs = glob(os.path.join(val_dir, pat))\n",
    "    if val_imgs:\n",
    "        break\n",
    "val_imgs = val_imgs[:6]\n",
    "print('Previewing', len(val_imgs), 'images from', val_dir)\n",
    "\n",
    "\n",
    "def _to_numpy(x):\n",
    "    try:\n",
    "        import torch\n",
    "        if isinstance(x, np.ndarray):\n",
    "            return x\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return x.detach().cpu().numpy()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return np.asarray(x)\n",
    "\n",
    "\n",
    "def _order_points_four(pts: np.ndarray) -> np.ndarray:\n",
    "    pts = np.asarray(pts, dtype='float32')\n",
    "    if pts.shape[0] != 4:\n",
    "        return pts\n",
    "    s = pts.sum(axis=1)\n",
    "    diff = pts[:, 1] - pts[:, 0]\n",
    "    tl = pts[np.argmin(s)]\n",
    "    br = pts[np.argmax(s)]\n",
    "    tr = pts[np.argmin(diff)]\n",
    "    bl = pts[np.argmax(diff)]\n",
    "    return np.array([tl, tr, br, bl], dtype='float32')\n",
    "\n",
    "\n",
    "for imgp in val_imgs:\n",
    "    im = cv2.imread(imgp)\n",
    "    res = model.predict(source=im, imgsz=640, conf=0.5, iou=0.6, classes=[0], verbose=False, max_det=50)[0]\n",
    "\n",
    "    # Draw boxes\n",
    "    if res.boxes is not None and len(res.boxes) > 0:\n",
    "        for b in res.boxes:\n",
    "            x1,y1,x2,y2 = b.xyxy[0].int().cpu().tolist()\n",
    "            cv2.rectangle(im, (x1,y1), (x2,y2), (0,255,0), 2)\n",
    "\n",
    "    # Draw keypoints with CPU conversion and polygon\n",
    "    if getattr(res, 'keypoints', None) is not None and res.keypoints.xy is not None:\n",
    "        kxy = _to_numpy(res.keypoints.xy)\n",
    "        kcf = _to_numpy(getattr(res.keypoints, 'conf', None)) if getattr(res.keypoints, 'conf', None) is not None else None\n",
    "        for i in range(kxy.shape[0]):\n",
    "            pts = kxy[i]\n",
    "            if pts is None or pts.shape[0] < 4:\n",
    "                continue\n",
    "            mask = np.ones((pts.shape[0],), dtype=bool)\n",
    "            if kcf is not None:\n",
    "                mask = kcf[i] >= 0.1\n",
    "            pts_vis = pts[mask]\n",
    "            if pts_vis.shape[0] >= 4:\n",
    "                pts4 = pts_vis[:4]\n",
    "                ordered = _order_points_four(pts4)\n",
    "                poly = ordered.reshape((-1,1,2)).astype(int)\n",
    "                cv2.polylines(im, [poly], isClosed=True, color=(0,0,255), thickness=2)\n",
    "                for px, py in ordered:\n",
    "                    cv2.circle(im, (int(px), int(py)), 4, (0,255,255), -1)\n",
    "            else:\n",
    "                for px, py in pts[:4]:\n",
    "                    cv2.circle(im, (int(px), int(py)), 4, (0,255,255), -1)\n",
    "\n",
    "    cv2.imshow('preview', im)\n",
    "    if cv2.waitKey(0) & 0xFF == ord('q'):\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4912ac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.a) Quick sanity checks for validation split (to explain empty metrics warnings)\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import yaml, glob, os\n",
    "\n",
    "# Load dataset config\n",
    "with open('./dataset/data.yaml', 'r') as f:\n",
    "    data_cfg = yaml.safe_load(f)\n",
    "\n",
    "base = Path(data_cfg.get('path', './dataset')).resolve()\n",
    "val_rel = data_cfg.get('val')\n",
    "val_dir = (base / val_rel).resolve() if val_rel else (base / 'images' / 'val').resolve()\n",
    "\n",
    "# Try to locate labels/val\n",
    "label_dirs = [base / 'labels' / 'val', base / 'val' / 'labels']\n",
    "labels_dir = next((p for p in label_dirs if p.exists()), None)\n",
    "\n",
    "# Collect images\n",
    "IMG_EXTS = ('*.jpg','*.jpeg','*.png','*.bmp','*.webp')\n",
    "val_imgs = []\n",
    "for ext in IMG_EXTS:\n",
    "    val_imgs += glob.glob(str(val_dir / ext))\n",
    "val_imgs = sorted(set(val_imgs))\n",
    "\n",
    "# Collect labels\n",
    "label_files = []\n",
    "if labels_dir and labels_dir.exists():\n",
    "    label_files = sorted(glob.glob(str(labels_dir / '*.txt')))\n",
    "\n",
    "# Pairing check (image <-> label)\n",
    "img_stems = {Path(p).stem for p in val_imgs}\n",
    "lab_stems = {Path(p).stem for p in label_files}\n",
    "missing_labels = sorted(img_stems - lab_stems)[:20]\n",
    "missing_images = sorted(lab_stems - img_stems)[:20]\n",
    "\n",
    "# Label content check (YOLO-pose expects: cls cx cy w h + 3*K)\n",
    "valid_pose_lines = 0\n",
    "k_counter = Counter()\n",
    "empty_label_files = 0\n",
    "kpt_only_lines = 0\n",
    "\n",
    "for fp in label_files[:2000]:\n",
    "    try:\n",
    "        with open(fp, 'r') as f:\n",
    "            lines = [ln.strip() for ln in f if ln.strip()]\n",
    "        if not lines:\n",
    "            empty_label_files += 1\n",
    "            continue\n",
    "        for ln in lines[:4]:\n",
    "            parts = ln.split()\n",
    "            if len(parts) >= 8 and (len(parts) - 5) % 3 == 0:\n",
    "                valid_pose_lines += 1\n",
    "                Kcand = (len(parts) - 5) // 3\n",
    "                k_counter[Kcand] += 1\n",
    "            elif len(parts) >= 3 and (len(parts) - 1) % 2 == 0:\n",
    "                # likely keypoints-only (no bbox) -> will yield empty detection metrics\n",
    "                kpt_only_lines += 1\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "print('Validation images:', len(val_imgs))\n",
    "print('Validation label files:', len(label_files))\n",
    "print('Empty label files:', empty_label_files)\n",
    "print('Sample missing label stems (first 20):', missing_labels)\n",
    "print('Sample labels without matching images (first 20):', missing_images)\n",
    "print('Valid YOLO-pose lines seen:', valid_pose_lines, '| kpt-only lines (no bbox):', kpt_only_lines)\n",
    "print('Inferred K candidates (from labels):', dict(k_counter))\n",
    "\n",
    "if len(val_imgs) == 0:\n",
    "    print('[Hint] No validation images. Metrics arrays will be empty -> warnings. Ensure data.yaml val points to images/val and files exist.')\n",
    "elif len(label_files) == 0:\n",
    "    print('[Hint] No validation labels found. Place .txt under labels/val with YOLO-pose format (cls cx cy w h + 3*K).')\n",
    "elif valid_pose_lines == 0 and kpt_only_lines > 0:\n",
    "    print('[Hint] Labels look keypoints-only (no bbox). Ultralytics pose expects bbox + keypoints; add boxes or convert labels.')\n",
    "elif missing_labels:\n",
    "    print('[Hint] Some images have no corresponding label .txt. Create labels or remove those images from val set.')\n",
    "else:\n",
    "    print('Sanity checks look OK. Early-epoch warnings can still appear if the model predicts nothing yet; they usually disappear later.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03813597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.b) Auto-fix val labels: convert keypoints-only -> bbox+keypoints, create empty labels for unpaired images\n",
    "from pathlib import Path\n",
    "import yaml, os, glob, shutil\n",
    "\n",
    "# Settings\n",
    "DO_CREATE_EMPTY_LABELS = True   # create empty .txt for images without labels\n",
    "VIS_DEFAULT = 2                 # default visibility if missing in kpt-only labels (0=not labeled, 1=occluded, 2=visible)\n",
    "FLOAT_FMT = '{:.6f}'.format\n",
    "\n",
    "# Load dataset config and resolve dirs\n",
    "with open('./dataset/data.yaml', 'r') as f:\n",
    "    data_cfg = yaml.safe_load(f)\n",
    "base = Path(data_cfg.get('path', './dataset')).resolve()\n",
    "val_rel = data_cfg.get('val')\n",
    "val_dir = (base / val_rel).resolve() if val_rel else (base / 'images' / 'val').resolve()\n",
    "label_dirs = [base / 'labels' / 'val', base / 'val' / 'labels']\n",
    "labels_dir = next((p for p in label_dirs if p.exists()), None)\n",
    "assert val_dir.exists(), f\"Val images dir not found: {val_dir}\"\n",
    "assert labels_dir is not None, \"Could not find labels/val folder. Create it under dataset/labels/val or dataset/val/labels.\"\n",
    "\n",
    "# Backup dir\n",
    "backup_dir = base / 'labels_backup_before_pose_fix' / 'val'\n",
    "backup_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "IMG_EXTS = ('.jpg','.jpeg','.png','.bmp','.webp')\n",
    "\n",
    "# Build index of val images by stem\n",
    "img_map = {}\n",
    "for p in val_dir.iterdir():\n",
    "    if p.suffix.lower() in IMG_EXTS:\n",
    "        img_map[p.stem] = p\n",
    "\n",
    "# List label files\n",
    "label_files = sorted(glob.glob(str(labels_dir / '*.txt')))\n",
    "lab_stems = {Path(p).stem for p in label_files}\n",
    "img_stems = set(img_map.keys())\n",
    "\n",
    "# Create empty labels for images without labels (optional)\n",
    "created_empty = 0\n",
    "if DO_CREATE_EMPTY_LABELS:\n",
    "    for stem in sorted(img_stems - lab_stems):\n",
    "        outp = labels_dir / f\"{stem}.txt\"\n",
    "        if not outp.exists():\n",
    "            outp.write_text('')\n",
    "            created_empty += 1\n",
    "\n",
    "# Helper to detect if coordinates are normalized [0,1]\n",
    "def _coords_look_normalized(vals):\n",
    "    # if any coord > 1.0 or < 0.0, likely absolute pixels\n",
    "    return all(0.0 <= v <= 1.0 for v in vals)\n",
    "\n",
    "# Convert kpt-only lines to bbox+keypoints\n",
    "converted_files = 0\n",
    "converted_lines = 0\n",
    "skipped_lines_noimg = 0\n",
    "\n",
    "for lfp in label_files:\n",
    "    stem = Path(lfp).stem\n",
    "    with open(lfp, 'r') as f:\n",
    "        lines = [ln.strip() for ln in f]\n",
    "    if not any(ln for ln in lines):\n",
    "        continue\n",
    "\n",
    "    changed = False\n",
    "    new_lines = []\n",
    "    for ln in lines:\n",
    "        if not ln.strip():\n",
    "            new_lines.append(ln)\n",
    "            continue\n",
    "        parts = ln.split()\n",
    "        # full pose format: cls cx cy w h + 3*K\n",
    "        if len(parts) >= 8 and (len(parts) - 5) % 3 == 0:\n",
    "            new_lines.append(ln)\n",
    "            continue\n",
    "        # keypoints-only: cls x1 y1 x2 y2 ...\n",
    "        if len(parts) >= 3 and (len(parts) - 1) % 2 == 0:\n",
    "            cls_id = parts[0]\n",
    "            coords = list(map(float, parts[1:]))\n",
    "            xs = coords[0::2]\n",
    "            ys = coords[1::2]\n",
    "            normalized = _coords_look_normalized(xs + ys)\n",
    "            # If not normalized, try to normalize using image size\n",
    "            if not normalized:\n",
    "                im = img_map.get(stem)\n",
    "                if im is None:\n",
    "                    # cannot normalize without image\n",
    "                    new_lines.append(ln)\n",
    "                    skipped_lines_noimg += 1\n",
    "                    continue\n",
    "                try:\n",
    "                    import cv2\n",
    "                    im0 = cv2.imread(str(im))\n",
    "                    if im0 is None:\n",
    "                        new_lines.append(ln)\n",
    "                        skipped_lines_noimg += 1\n",
    "                        continue\n",
    "                    h, w = im0.shape[:2]\n",
    "                    xs = [x / w for x in xs]\n",
    "                    ys = [y / h for y in ys]\n",
    "                    normalized = True\n",
    "                except Exception:\n",
    "                    new_lines.append(ln)\n",
    "                    skipped_lines_noimg += 1\n",
    "                    continue\n",
    "            # compute bbox from normalized kpts\n",
    "            min_x, max_x = max(0.0, min(xs)), min(1.0, max(xs))\n",
    "            min_y, max_y = max(0.0, min(ys)), min(1.0, max(ys))\n",
    "            bw = max(1e-6, max_x - min_x)\n",
    "            bh = max(1e-6, max_y - min_y)\n",
    "            cx = (min_x + max_x) / 2.0\n",
    "            cy = (min_y + max_y) / 2.0\n",
    "            # build v flags (default visible)\n",
    "            K = len(xs)\n",
    "            kpts_fmt = []\n",
    "            for x, y in zip(xs, ys):\n",
    "                kpts_fmt.extend([FLOAT_FMT(x), FLOAT_FMT(y), str(VIS_DEFAULT)])\n",
    "            new_ln = ' '.join([cls_id, FLOAT_FMT(cx), FLOAT_FMT(cy), FLOAT_FMT(bw), FLOAT_FMT(bh)] + kpts_fmt)\n",
    "            new_lines.append(new_ln)\n",
    "            converted_lines += 1\n",
    "            changed = True\n",
    "        else:\n",
    "            new_lines.append(ln)\n",
    "\n",
    "    if changed:\n",
    "        # backup once per file\n",
    "        bk = backup_dir / f\"{Path(lfp).name}\"\n",
    "        if not bk.exists():\n",
    "            shutil.copy2(lfp, bk)\n",
    "        with open(lfp, 'w') as f:\n",
    "            f.write('\\n'.join(new_lines).strip() + '\\n')\n",
    "        converted_files += 1\n",
    "\n",
    "print('Auto-fix summary:')\n",
    "print(' - Created empty labels:', created_empty)\n",
    "print(' - Converted files:', converted_files)\n",
    "print(' - Converted lines:', converted_lines)\n",
    "print(' - Skipped lines (no image to normalize):', skipped_lines_noimg)\n",
    "print('Backup of originals:', backup_dir)\n",
    "print('Tip: Re-run the sanity check cell (3.a) to verify improvements, then retrain.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad875ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.c) Clear Ultralytics label caches (so updated labels take effect)\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "root = Path('./dataset')\n",
    "for cache_name in ['train.cache', 'val.cache']:\n",
    "    cp = root / 'labels' / cache_name\n",
    "    if cp.exists():\n",
    "        try:\n",
    "            os.remove(cp)\n",
    "            print('Removed cache:', cp)\n",
    "        except Exception as e:\n",
    "            print('Failed to remove cache', cp, e)\n",
    "    else:\n",
    "        print('Cache not found (ok):', cp)\n",
    "print('Caches cleared. Re-run sanity check (3.a) if desired, then train (3).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38b0534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.d) Inspect label column widths (train & val) and expected K\n",
    "from pathlib import Path\n",
    "import yaml, glob, os\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Load dataset config\n",
    "with open('./dataset/data.yaml', 'r') as f:\n",
    "    data_cfg = yaml.safe_load(f)\n",
    "base = Path(data_cfg.get('path', './dataset')).resolve()\n",
    "\n",
    "# Resolve split dirs\n",
    "split_img_dirs = {\n",
    "    'train': (base / (data_cfg.get('train') or 'images/train')).resolve(),\n",
    "    'val':   (base / (data_cfg.get('val')   or 'images/val')).resolve(),\n",
    "}\n",
    "split_lbl_dirs = {\n",
    "    'train': next((p for p in [base/'labels'/'train', base/'train'/'labels'] if p.exists()), None),\n",
    "    'val':   next((p for p in [base/'labels'/'val',   base/'val'/'labels']   if p.exists()), None),\n",
    "}\n",
    "\n",
    "print('Label directories:', {k: str(v) if v else None for k,v in split_lbl_dirs.items()})\n",
    "\n",
    "# Scan labels to compute per-line width and infer K per split\n",
    "summary = {}\n",
    "offenders = defaultdict(list)\n",
    "\n",
    "for split, ldir in split_lbl_dirs.items():\n",
    "    widths = Counter()\n",
    "    k_counts = Counter()\n",
    "    det_only = 0\n",
    "    kpt_only = 0\n",
    "    invalid = 0\n",
    "    files = []\n",
    "    if ldir and ldir.exists():\n",
    "        files = sorted(glob.glob(str(ldir / '*.txt')))\n",
    "    for lfp in files:\n",
    "        try:\n",
    "            with open(lfp, 'r') as f:\n",
    "                lines = [ln.strip() for ln in f if ln.strip()]\n",
    "        except Exception:\n",
    "            continue\n",
    "        for ln in lines:\n",
    "            parts = ln.split()\n",
    "            n = len(parts)\n",
    "            if n == 5:\n",
    "                det_only += 1\n",
    "                widths[n] += 1\n",
    "                offenders[(split, 'detection_only')].append(lfp)\n",
    "            elif n >= 8 and (n - 5) % 3 == 0:\n",
    "                K = (n - 5) // 3\n",
    "                k_counts[K] += 1\n",
    "                widths[n] += 1\n",
    "            elif n >= 3 and (n - 1) % 2 == 0:\n",
    "                kpt_only += 1\n",
    "                widths[n] += 1\n",
    "                offenders[(split, 'kpt_only_no_bbox')].append(lfp)\n",
    "            else:\n",
    "                invalid += 1\n",
    "                widths[n] += 1\n",
    "                offenders[(split, 'invalid_width')].append(lfp)\n",
    "    summary[split] = dict(\n",
    "        files=len(files),\n",
    "        widths=dict(widths),\n",
    "        inferred_K=dict(k_counts),\n",
    "        detection_only=det_only,\n",
    "        kpt_only_no_bbox=kpt_only,\n",
    "        invalid=invalid,\n",
    "    )\n",
    "\n",
    "print('Summary (by split):')\n",
    "for split, info in summary.items():\n",
    "    print(f'  {split}:')\n",
    "    for k, v in info.items():\n",
    "        print(f'    {k}: {v}')\n",
    "\n",
    "# If a dominant K exists, compute expected columns = 5 + 3*K\n",
    "all_k = Counter()\n",
    "for s in summary.values():\n",
    "    for k, c in s['inferred_K'].items():\n",
    "        all_k[k] += c\n",
    "expected_K = max(all_k, key=all_k.get) if all_k else None\n",
    "expected_cols = 5 + 3*expected_K if expected_K is not None else None\n",
    "print('Inferred dominant K across splits:', expected_K, '| expected columns per line:', expected_cols)\n",
    "\n",
    "# Show a few offender files per type\n",
    "for (split, typ), lst in offenders.items():\n",
    "    if lst:\n",
    "        print(f'Offenders [{split}::{typ}] (up to 10):')\n",
    "        for p in sorted(set(lst))[:10]:\n",
    "            print('  -', os.path.basename(p))\n",
    "\n",
    "print('Tip: For pose, each line must be: cls cx cy w h + 3*K (x y v) with K keypoints, normalized [0,1].')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b3d8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.e) Normalize labels to K=4 pose format across train & val\n",
    "from pathlib import Path\n",
    "import yaml, os, glob, shutil\n",
    "\n",
    "# Settings\n",
    "VIS_DEFAULT = 2                 # visibility for generated keypoints\n",
    "FLOAT_FMT = '{:.6f}'.format\n",
    "\n",
    "# Load dataset config and resolve dirs\n",
    "with open('./dataset/data.yaml', 'r') as f:\n",
    "    data_cfg = yaml.safe_load(f)\n",
    "base = Path(data_cfg.get('path', './dataset')).resolve()\n",
    "\n",
    "# Resolve split folders\n",
    "split_to_img = {\n",
    "    'train': (base / (data_cfg.get('train') or 'images/train')).resolve(),\n",
    "    'val':   (base / (data_cfg.get('val')   or 'images/val')).resolve(),\n",
    "}\n",
    "split_to_lbl = {\n",
    "    'train': next((p for p in [base/'labels'/'train', base/'train'/'labels'] if p.exists()), None),\n",
    "    'val':   next((p for p in [base/'labels'/'val',   base/'val'/'labels']   if p.exists()), None),\n",
    "}\n",
    "\n",
    "# Backups\n",
    "backup_root = base / 'labels_backup_before_pose_fix'\n",
    "\n",
    "IMG_EXTS = {'.jpg','.jpeg','.png','.bmp','.webp'}\n",
    "\n",
    "def bbox_corners(cx, cy, w, h):\n",
    "    x1 = max(0.0, cx - w/2)\n",
    "    y1 = max(0.0, cy - h/2)\n",
    "    x2 = min(1.0, cx + w/2)\n",
    "    y2 = min(1.0, cy + h/2)\n",
    "    # Order: tl, tr, br, bl\n",
    "    return [(x1,y1),(x2,y1),(x2,y2),(x1,y2)]\n",
    "\n",
    "def coords_look_normalized(vals):\n",
    "    return all(0.0 <= v <= 1.0 for v in vals)\n",
    "\n",
    "# Build image maps for normalization when needed\n",
    "split_img_map = {}\n",
    "for split, idir in split_to_img.items():\n",
    "    imap = {}\n",
    "    if idir and idir.exists():\n",
    "        for p in idir.iterdir():\n",
    "            if p.suffix.lower() in IMG_EXTS:\n",
    "                imap[p.stem] = p\n",
    "    split_img_map[split] = imap\n",
    "\n",
    "stats = {\n",
    "    'train_det_to_pose': 0,\n",
    "    'train_kpt_only_to_pose': 0,\n",
    "    'val_k2_to_k4': 0,\n",
    "    'files_touched': 0,\n",
    "}\n",
    "\n",
    "for split in ('train','val'):\n",
    "    ldir = split_to_lbl[split]\n",
    "    if not ldir or not ldir.exists():\n",
    "        continue\n",
    "    (backup_root/split).mkdir(parents=True, exist_ok=True)\n",
    "    files = sorted(glob.glob(str(ldir / '*.txt')))\n",
    "    for lfp in files:\n",
    "        stem = Path(lfp).stem\n",
    "        with open(lfp, 'r') as f:\n",
    "            lines = [ln.rstrip('\\n') for ln in f]\n",
    "        new_lines = []\n",
    "        changed = False\n",
    "        for ln in lines:\n",
    "            if not ln.strip():\n",
    "                new_lines.append(ln)\n",
    "                continue\n",
    "            parts = ln.split()\n",
    "            n = len(parts)\n",
    "            # Detection-only -> add 4 bbox-corner keypoints\n",
    "            if n == 5:\n",
    "                cls_id, cx, cy, w, h = parts\n",
    "                cx = float(cx); cy = float(cy); w = float(w); h = float(h)\n",
    "                kpts = []\n",
    "                for x,y in bbox_corners(cx, cy, w, h):\n",
    "                    kpts.extend([FLOAT_FMT(x), FLOAT_FMT(y), str(VIS_DEFAULT)])\n",
    "                new_lines.append(' '.join([cls_id, FLOAT_FMT(cx), FLOAT_FMT(cy), FLOAT_FMT(w), FLOAT_FMT(h)] + kpts))\n",
    "                changed = True\n",
    "                if split == 'train':\n",
    "                    stats['train_det_to_pose'] += 1\n",
    "                continue\n",
    "            # Pose with K=2 -> replace keypoints with bbox corners to make K=4\n",
    "            if n == 11 and (n - 5) % 3 == 0:\n",
    "                cls_id, cx, cy, w, h = parts[:5]\n",
    "                cx = float(cx); cy = float(cy); w = float(w); h = float(h)\n",
    "                kpts = []\n",
    "                for x,y in bbox_corners(cx, cy, w, h):\n",
    "                    kpts.extend([FLOAT_FMT(x), FLOAT_FMT(y), str(VIS_DEFAULT)])\n",
    "                new_lines.append(' '.join([cls_id, FLOAT_FMT(cx), FLOAT_FMT(cy), FLOAT_FMT(w), FLOAT_FMT(h)] + kpts))\n",
    "                changed = True\n",
    "                if split == 'val':\n",
    "                    stats['val_k2_to_k4'] += 1\n",
    "                continue\n",
    "            # Keypoints-only -> build bbox and add vis flags\n",
    "            if n >= 3 and (n - 1) % 2 == 0:\n",
    "                cls_id = parts[0]\n",
    "                coords = list(map(float, parts[1:]))\n",
    "                xs = coords[0::2]\n",
    "                ys = coords[1::2]\n",
    "                normalized = coords_look_normalized(xs + ys)\n",
    "                if not normalized:\n",
    "                    im = split_img_map[split].get(stem)\n",
    "                    if im is not None:\n",
    "                        try:\n",
    "                            import cv2\n",
    "                            im0 = cv2.imread(str(im))\n",
    "                            h0, w0 = im0.shape[:2]\n",
    "                            xs = [x / w0 for x in xs]\n",
    "                            ys = [y / h0 for y in ys]\n",
    "                            normalized = True\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                if normalized:\n",
    "                    min_x, max_x = max(0.0, min(xs)), min(1.0, max(xs))\n",
    "                    min_y, max_y = max(0.0, min(ys)), min(1.0, max(ys))\n",
    "                    bw = max(1e-6, max_x - min_x)\n",
    "                    bh = max(1e-6, max_y - min_y)\n",
    "                    cx = (min_x + max_x) / 2.0\n",
    "                    cy = (min_y + max_y) / 2.0\n",
    "                    # Generate 4 bbox-corner keypoints regardless of original K to keep consistent K=4\n",
    "                    kpts = []\n",
    "                    for x,y in bbox_corners(cx, cy, bw, bh):\n",
    "                        kpts.extend([FLOAT_FMT(x), FLOAT_FMT(y), str(VIS_DEFAULT)])\n",
    "                    new_lines.append(' '.join([cls_id, FLOAT_FMT(cx), FLOAT_FMT(cy), FLOAT_FMT(bw), FLOAT_FMT(bh)] + kpts))\n",
    "                    changed = True\n",
    "                    if split == 'train':\n",
    "                        stats['train_kpt_only_to_pose'] += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    # cannot normalize; keep original line\n",
    "                    new_lines.append(ln)\n",
    "                    continue\n",
    "            # Already pose with K>=4 or other -> keep as-is\n",
    "            new_lines.append(ln)\n",
    "        if changed:\n",
    "            bk = backup_root / split / f'{Path(lfp).name}'\n",
    "            if not bk.exists():\n",
    "                shutil.copy2(lfp, bk)\n",
    "            with open(lfp, 'w') as f:\n",
    "                f.write('\\n'.join(new_lines).strip() + '\\n')\n",
    "            stats['files_touched'] += 1\n",
    "\n",
    "print('Normalize summary:')\n",
    "for k,v in stats.items():\n",
    "    print(f' - {k}: {v}')\n",
    "print('Backups saved under:', backup_root)\n",
    "print('Tip: Run cell 3.c to clear caches, then re-run 3.d to verify widths are now 17 across splits, and retrain (3).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd66582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.f) Fix invalid class IDs (e.g., class 1 when dataset has only class 0)\n",
    "from pathlib import Path\n",
    "import yaml, glob, shutil\n",
    "from collections import Counter\n",
    "\n",
    "# Load dataset config\n",
    "with open('./dataset/data.yaml', 'r') as f:\n",
    "    data_cfg = yaml.safe_load(f)\n",
    "\n",
    "base = Path(data_cfg.get('path', './dataset')).resolve()\n",
    "# Determine number of classes from names\n",
    "names = data_cfg.get('names', ['plate'])\n",
    "if isinstance(names, dict):\n",
    "    # names: {0: 'plate'}\n",
    "    class_count = len(names)\n",
    "elif isinstance(names, (list, tuple)):\n",
    "    class_count = len(names)\n",
    "else:\n",
    "    class_count = 1\n",
    "\n",
    "valid_min, valid_max = 0, max(0, class_count - 1)\n",
    "\n",
    "# Resolve split folders\n",
    "split_to_lbl = {\n",
    "    'train': next((p for p in [base/'labels'/'train', base/'train'/'labels'] if p.exists()), None),\n",
    "    'val':   next((p for p in [base/'labels'/'val',   base/'val'/'labels']   if p.exists()), None),\n",
    "}\n",
    "\n",
    "backup_root = base / 'labels_backup_before_pose_fix' / 'class_fix'\n",
    "backup_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "stats_split = {}\n",
    "\n",
    "def parse_cls(tok: str):\n",
    "    try:\n",
    "        # handle '0', '0.0', etc.\n",
    "        return int(float(tok))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "for split, ldir in split_to_lbl.items():\n",
    "    if not ldir or not ldir.exists():\n",
    "        continue\n",
    "    files = sorted(glob.glob(str(ldir / '*.txt')))\n",
    "    changed_files = 0\n",
    "    total_lines = 0\n",
    "    changed_lines = 0\n",
    "    invalid_tokens = 0\n",
    "    cls_hist_before = Counter()\n",
    "    cls_hist_after = Counter()\n",
    "\n",
    "    (backup_root / split).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for lfp in files:\n",
    "        with open(lfp, 'r') as f:\n",
    "            lines = [ln.rstrip('\\n') for ln in f]\n",
    "        new_lines = []\n",
    "        file_changed = False\n",
    "        for ln in lines:\n",
    "            if not ln.strip():\n",
    "                new_lines.append(ln)\n",
    "                continue\n",
    "            parts = ln.split()\n",
    "            if len(parts) < 5:\n",
    "                # not a proper YOLO line; keep as-is\n",
    "                new_lines.append(ln)\n",
    "                continue\n",
    "            cid = parse_cls(parts[0])\n",
    "            total_lines += 1\n",
    "            if cid is None:\n",
    "                invalid_tokens += 1\n",
    "                cid = 0\n",
    "            cls_hist_before[cid] += 1\n",
    "            new_cid = min(max(cid, valid_min), valid_max)\n",
    "            if new_cid != cid:\n",
    "                parts[0] = str(new_cid)\n",
    "                file_changed = True\n",
    "                changed_lines += 1\n",
    "            new_lines.append(' '.join(parts))\n",
    "            cls_hist_after[new_cid] += 1\n",
    "        if file_changed:\n",
    "            bk = backup_root / split / Path(lfp).name\n",
    "            if not bk.exists():\n",
    "                shutil.copy2(lfp, bk)\n",
    "            with open(lfp, 'w') as f:\n",
    "                f.write('\\n'.join(new_lines).strip() + '\\n')\n",
    "            changed_files += 1\n",
    "\n",
    "    stats_split[split] = {\n",
    "        'files': len(files),\n",
    "        'changed_files': changed_files,\n",
    "        'total_lines': total_lines,\n",
    "        'changed_lines': changed_lines,\n",
    "        'invalid_tokens': invalid_tokens,\n",
    "        'cls_hist_before': dict(cls_hist_before),\n",
    "        'cls_hist_after': dict(cls_hist_after),\n",
    "    }\n",
    "\n",
    "print('Class-fix summary (valid IDs =', f'{valid_min}-{valid_max}', '):')\n",
    "for split, st in stats_split.items():\n",
    "    print(f'  {split}:')\n",
    "    for k, v in st.items():\n",
    "        if isinstance(v, dict):\n",
    "            print(f'    {k}: {dict(sorted(v.items()))}')\n",
    "        else:\n",
    "            print(f'    {k}: {v}')\n",
    "print('Backups saved under:', backup_root)\n",
    "print('Tip: Run cell 3.c to clear caches, then retrain (3).')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
