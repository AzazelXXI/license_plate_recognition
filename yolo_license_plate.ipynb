{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9653af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Dataset scan + build data.yaml for pose (robust scan)\n",
    "from pathlib import Path\n",
    "import os, yaml\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "\n",
    "root = Path('./dataset')\n",
    "assert root.exists(), \"Dataset folder './dataset' not found. Please scp/rsync it first.\"\n",
    "\n",
    "# Find YOLO label .txt anywhere under labels/ (supports both layouts)\n",
    "label_txts = []\n",
    "label_txts += glob(str(root / 'labels' / '**' / '*.txt'), recursive=True)          # type-first: dataset/labels/train/*.txt\n",
    "label_txts += glob(str(root / '*' / 'labels' / '**' / '*.txt'), recursive=True)    # split-first: dataset/train/labels/*.txt\n",
    "label_txts = sorted(set(label_txts))\n",
    "print(f\"Found {len(label_txts)} label files under 'labels' folders\")\n",
    "\n",
    "if len(label_txts) == 0:\n",
    "    # Last resort: search any .txt and warn\n",
    "    label_txts = glob(str(root / '**' / '*.txt'), recursive=True)\n",
    "    print(f\"Fallback: found {len(label_txts)} .txt files total\")\n",
    "    assert len(label_txts)>0, \"No label .txt files found anywhere in './dataset'. Expected YOLO-pose labels under .../labels/...\"\n",
    "\n",
    "# Robustly infer keypoints K by majority across many labels\n",
    "kp_counter = Counter()\n",
    "for i, fp in enumerate(label_txts[:2000]):  # sample up to 2000 files\n",
    "    try:\n",
    "        with open(fp, 'r') as f:\n",
    "            for j, ln in enumerate(f):\n",
    "                ln = ln.strip()\n",
    "                if not ln:\n",
    "                    continue\n",
    "                parts = ln.split()\n",
    "                # prefer full pose labels: cls cx cy w h + 3*K\n",
    "                if len(parts) >= 8 and (len(parts) - 5) % 3 == 0:\n",
    "                    Kcand = (len(parts) - 5) // 3\n",
    "                    kp_counter[Kcand] += 1\n",
    "                # stop early per file after a couple of lines\n",
    "                if j >= 2:\n",
    "                    break\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "if not kp_counter:\n",
    "    # Fallback: detect K from kpt-only (cls x1 y1 x2 y2 ...)\n",
    "    kpt_only_counter = Counter()\n",
    "    for i, fp in enumerate(label_txts[:2000]):\n",
    "        try:\n",
    "            with open(fp, 'r') as f:\n",
    "                for j, ln in enumerate(f):\n",
    "                    ln = ln.strip()\n",
    "                    if not ln:\n",
    "                        continue\n",
    "                    parts = ln.split()\n",
    "                    if len(parts) >= 3 and (len(parts) - 1) % 2 == 0:\n",
    "                        Kcand = (len(parts) - 1) // 2\n",
    "                        kpt_only_counter[Kcand] += 1\n",
    "                    if j >= 2:\n",
    "                        break\n",
    "        except Exception:\n",
    "            continue\n",
    "    assert kpt_only_counter, 'Could not infer keypoints K from labels.'\n",
    "    K = max(kpt_only_counter, key=kpt_only_counter.get)\n",
    "else:\n",
    "    K = max(kp_counter, key=kp_counter.get)\n",
    "\n",
    "print(f\"Inferred keypoints K (majority) = {K}\")\n",
    "\n",
    "# Heuristics to resolve images dirs for each split\n",
    "def choose_images_dir(split: str):\n",
    "    candidates = [\n",
    "        root / 'images' / split,       # type-first\n",
    "        root / split / 'images',       # split-first\n",
    "    ]\n",
    "    # derive from labels/<split>\n",
    "    split_label_dirs = []\n",
    "    for p in label_txts:\n",
    "        p_norm = p.replace('\\\\', '/').lower()\n",
    "        if f\"/labels/{split}/\" in p_norm:\n",
    "            split_label_dirs.append(Path(p).parent)\n",
    "    if split_label_dirs:\n",
    "        ldir = split_label_dirs[0]\n",
    "        candidates += [\n",
    "            ldir.parent / 'images',                  # .../train/images\n",
    "            ldir.parent.parent / 'images' / ldir.name # .../images/train\n",
    "        ]\n",
    "    for c in candidates:\n",
    "        if c.exists():\n",
    "            return str(c)\n",
    "    return None\n",
    "\n",
    "train_path = choose_images_dir('train')\n",
    "val_path = choose_images_dir('val')\n",
    "test_path = choose_images_dir('test')\n",
    "print('Images dirs (raw):', {'train': train_path, 'val': val_path, 'test': test_path})\n",
    "\n",
    "# Normalize to be RELATIVE to dataset root if possible, else absolute\n",
    "root_abs = root.resolve()\n",
    "\n",
    "def to_rel_or_abs(p):\n",
    "    if not p:\n",
    "        return None\n",
    "    p_abs = Path(p)\n",
    "    try:\n",
    "        p_abs = p_abs.resolve()\n",
    "    except Exception:\n",
    "        p_abs = (root / p).resolve()\n",
    "    try:\n",
    "        return str(p_abs.relative_to(root_abs))\n",
    "    except ValueError:\n",
    "        return str(p_abs)\n",
    "\n",
    "train_rel = to_rel_or_abs(train_path)\n",
    "val_rel = to_rel_or_abs(val_path)\n",
    "test_rel = to_rel_or_abs(test_path)\n",
    "print('Images dirs (normalized):', {'train': train_rel, 'val': val_rel, 'test': test_rel})\n",
    "\n",
    "assert train_rel and val_rel, (\n",
    "    \"Could not locate images/train and/or images/val. Ensure YOLO structure like:\\n\"\n",
    "    \"- dataset/images/train & dataset/labels/train\\n\"\n",
    "    \"- dataset/train/images & dataset/train/labels\\n\"\n",
    ")\n",
    "\n",
    "# Build YAML for pose\n",
    "names = ['plate']\n",
    "skeleton = [[0,1],[1,2],[2,3],[3,0]] if K==4 else []\n",
    "flip_idx = list(range(K))\n",
    "\n",
    "data = {\n",
    "    'path': str(root_abs),\n",
    "    'train': train_rel,\n",
    "    'val': val_rel,\n",
    "    'test': test_rel,\n",
    "    'names': names,\n",
    "    'kpt_shape': [K, 3],\n",
    "    'skeleton': skeleton,\n",
    "    'flip_idx': flip_idx,\n",
    "}\n",
    "\n",
    "yaml_path = root/'data.yaml'\n",
    "with open(yaml_path, 'w') as f:\n",
    "    yaml.safe_dump(data, f, sort_keys=False)\n",
    "\n",
    "print('Wrote', yaml_path)\n",
    "print(yaml.safe_dump(data, sort_keys=False))\n",
    "\n",
    "# Clear label caches so Ultralytics rebuilds with the corrected kpt_shape\n",
    "for cache_name in ['train.cache', 'val.cache']:\n",
    "    cp = root / 'labels' / cache_name\n",
    "    if cp.exists():\n",
    "        try:\n",
    "            os.remove(cp)\n",
    "            print('Removed cache:', cp)\n",
    "        except Exception as e:\n",
    "            print('Failed to remove cache', cp, e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d29a507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5) Quick audit (10 files) + auto-fix labels to YOLOv8-Pose format if needed\n",
    "from pathlib import Path\n",
    "import os, shutil, glob\n",
    "\n",
    "labels_root = Path('./dataset/labels')\n",
    "assert labels_root.exists(), 'labels folder not found'\n",
    "\n",
    "# Sample ~10 label files from val to inspect\n",
    "samples = []\n",
    "for split in ['val', 'train']:\n",
    "    cand = sorted(glob.glob(str(labels_root / split / '*.txt')))\n",
    "    if cand:\n",
    "        samples += cand[:max(0, 10 - len(samples))]\n",
    "    if len(samples) >= 10:\n",
    "        break\n",
    "\n",
    "print('Sampling files:', len(samples))\n",
    "\n",
    "# Inspect helper\n",
    "def classify_format(tokens):\n",
    "    # tokens: list[str]\n",
    "    n = len(tokens)\n",
    "    if n >= 6 and (n - 5) % 3 == 0:\n",
    "        return 'yolo_pose_full'  # cls x y w h x1 y1 v1 ...\n",
    "    if n >= 3 and (n - 1) % 2 == 0:\n",
    "        return 'kpt_only'        # cls x1 y1 x2 y2 ... (no bbox, no v)\n",
    "    return 'unknown'\n",
    "\n",
    "bad_examples = []\n",
    "for fp in samples:\n",
    "    with open(fp, 'r') as f:\n",
    "        lines = [ln.strip() for ln in f.readlines() if ln.strip()]\n",
    "    for ln in lines[:2]:  # show at most 2 lines/file\n",
    "        toks = ln.split()\n",
    "        fmt = classify_format(toks)\n",
    "        if fmt != 'yolo_pose_full':\n",
    "            bad_examples.append((fp, ln, fmt))\n",
    "        else:\n",
    "            # quick sanity: visibility values should be integers 0/1/2\n",
    "            try:\n",
    "                # v1 begins at index 7 (0-based): [cls,cx,cy,w,h,x1,y1,v1,x2,y2,v2,...]\n",
    "                vis_vals = [float(v) for v in toks[7::3]]\n",
    "                if not all(v in (0, 1, 2) for v in vis_vals):\n",
    "                    bad_examples.append((fp, ln, 'invalid_visibility'))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "print('Found non-compliant examples (up to 10 shown):')\n",
    "for i, (fp, ln, fmt) in enumerate(bad_examples[:10], 1):\n",
    "    print(f'{i:02d}. {fp} | fmt={fmt} | line=\"{ln[:120]}\"')\n",
    "\n",
    "# If most samples are kpt_only, auto-convert the whole dataset in-place (with backup)\n",
    "needs_convert = any(fmt == 'kpt_only' for _, _, fmt in bad_examples) and not any(fmt == 'yolo_pose_full' for _, _, fmt in bad_examples)\n",
    "print('Needs conversion:', needs_convert)\n",
    "\n",
    "\n",
    "def convert_file_in_place(txt_path: Path, backup_dir: Path, expected_k: int | None = None) -> tuple[int, int]:\n",
    "    \"\"\"Convert kpt-only lines to YOLOv8-pose full format. Returns (ok_lines, total_lines).\"\"\"\n",
    "    with open(txt_path, 'r') as f:\n",
    "        lines = [ln.strip() for ln in f.readlines() if ln.strip()]\n",
    "\n",
    "    out_lines = []\n",
    "    ok, total = 0, 0\n",
    "    for ln in lines:\n",
    "        total += 1\n",
    "        toks = ln.split()\n",
    "        fmt = classify_format(toks)\n",
    "        if fmt == 'yolo_pose_full':\n",
    "            out_lines.append(ln)\n",
    "            ok += 1\n",
    "            continue\n",
    "        if fmt != 'kpt_only':\n",
    "            # skip malformed lines\n",
    "            continue\n",
    "        try:\n",
    "            cls = int(float(toks[0]))\n",
    "        except Exception:\n",
    "            continue\n",
    "        vals = list(map(float, toks[1:]))\n",
    "        if len(vals) % 2 != 0:\n",
    "            continue\n",
    "        K = len(vals) // 2\n",
    "        if expected_k is not None and K != expected_k:\n",
    "            # inconsistent K\n",
    "            continue\n",
    "        xs = vals[0::2]\n",
    "        ys = vals[1::2]\n",
    "        # clamp to [0,1]\n",
    "        xs = [min(1.0, max(0.0, x)) for x in xs]\n",
    "        ys = [min(1.0, max(0.0, y)) for y in ys]\n",
    "        x1, x2 = min(xs), max(xs)\n",
    "        y1, y2 = min(ys), max(ys)\n",
    "        w = max(x2 - x1, 1e-6)\n",
    "        h = max(y2 - y1, 1e-6)\n",
    "        cx = (x1 + x2) / 2.0\n",
    "        cy = (y1 + y2) / 2.0\n",
    "        # visibility: mark as 2 (labeled & visible)\n",
    "        kv = []\n",
    "        for i in range(K):\n",
    "            kv += [xs[i], ys[i], 2]\n",
    "        new_toks = [str(cls), f'{cx:.6f}', f'{cy:.6f}', f'{w:.6f}', f'{h:.6f}'] + [f'{v:.6f}' if isinstance(v, float) else str(v) for v in kv]\n",
    "        out_lines.append(' '.join(new_toks))\n",
    "        ok += 1\n",
    "\n",
    "    if ok:\n",
    "        # backup original once\n",
    "        backup_path = backup_dir / txt_path.name\n",
    "        if not backup_path.exists():\n",
    "            shutil.copy2(txt_path, backup_path)\n",
    "        with open(txt_path, 'w') as f:\n",
    "            f.write('\\n'.join(out_lines) + ('\\n' if out_lines else ''))\n",
    "    return ok, total\n",
    "\n",
    "if needs_convert:\n",
    "    backup_root = labels_root.parent / 'labels_backup_before_pose_fix'\n",
    "    backup_root.mkdir(exist_ok=True)\n",
    "    sum_ok = 0\n",
    "    sum_total = 0\n",
    "    # Infer K from first bad sample\n",
    "    K_guess = None\n",
    "    for _, ln, fmt in bad_examples:\n",
    "        if fmt == 'kpt_only':\n",
    "            toks = ln.split()\n",
    "            K_guess = (len(toks) - 1) // 2\n",
    "            break\n",
    "    print('Assumed K from samples =', K_guess)\n",
    "\n",
    "    for split in ['train', 'val']:\n",
    "        split_dir = labels_root / split\n",
    "        if not split_dir.exists():\n",
    "            continue\n",
    "        backup_dir = backup_root / split\n",
    "        backup_dir.mkdir(parents=True, exist_ok=True)\n",
    "        files = sorted(split_dir.glob('*.txt'))\n",
    "        for i, fp in enumerate(files, 1):\n",
    "            ok, total = convert_file_in_place(fp, backup_dir, expected_k=K_guess)\n",
    "            sum_ok += ok\n",
    "            sum_total += total\n",
    "            if i % 500 == 0:\n",
    "                print(f'Converted {i}/{len(files)} files...')\n",
    "    print(f'Converted lines: {sum_ok}/{sum_total}. Backup at: {backup_root}')\n",
    "\n",
    "    # Clear caches so Ultralytics rebuilds with new labels\n",
    "    for cache_name in ['train.cache', 'val.cache']:\n",
    "        cp = labels_root / cache_name\n",
    "        if cp.exists():\n",
    "            try:\n",
    "                os.remove(cp)\n",
    "                print('Removed cache:', cp)\n",
    "            except Exception as e:\n",
    "                print('Failed to remove cache', cp, e)\n",
    "else:\n",
    "    print('Labels appear to already be in YOLOv8-pose format for sampled files. No conversion applied.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb3c0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.6) Normalize class IDs to single class (0) and clear caches\n",
    "from pathlib import Path\n",
    "import os, glob\n",
    "\n",
    "labels_root = Path('./dataset/labels')\n",
    "assert labels_root.exists(), 'labels folder not found'\n",
    "\n",
    "changed_lines = 0\n",
    "changed_files = 0\n",
    "\n",
    "def normalize_classes(txt_path: Path) -> tuple[int, bool]:\n",
    "    with open(txt_path, 'r') as f:\n",
    "        lines = [ln.rstrip('\\n') for ln in f.readlines()]\n",
    "    out = []\n",
    "    changed = False\n",
    "    count = 0\n",
    "    for ln in lines:\n",
    "        if not ln.strip():\n",
    "            out.append(ln)\n",
    "            continue\n",
    "        toks = ln.split()\n",
    "        try:\n",
    "            cls = int(float(toks[0]))\n",
    "        except Exception:\n",
    "            out.append(ln)\n",
    "            continue\n",
    "        if cls != 0:\n",
    "            toks[0] = '0'\n",
    "            changed = True\n",
    "            count += 1\n",
    "        out.append(' '.join(toks))\n",
    "    if changed:\n",
    "        with open(txt_path, 'w') as f:\n",
    "            f.write('\\n'.join(out) + ('\\n' if out and out[-1] != '' else ''))\n",
    "    return count, changed\n",
    "\n",
    "for split in ['train', 'val']:\n",
    "    files = sorted((labels_root / split).glob('*.txt')) if (labels_root / split).exists() else []\n",
    "    for fp in files:\n",
    "        cnt, ch = normalize_classes(fp)\n",
    "        changed_lines += cnt\n",
    "        changed_files += int(ch)\n",
    "\n",
    "print(f'Normalized class IDs in {changed_files} files, {changed_lines} lines changed to class 0.')\n",
    "\n",
    "# Clear caches again\n",
    "for cache_name in ['train.cache', 'val.cache']:\n",
    "    cp = labels_root / cache_name\n",
    "    if cp.exists():\n",
    "        try:\n",
    "            os.remove(cp)\n",
    "            print('Removed cache:', cp)\n",
    "        except Exception as e:\n",
    "            print('Failed to remove cache', cp, e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c2501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Train\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "yaml_path = Path('./dataset/data.yaml')\n",
    "assert yaml_path.exists(), 'data.yaml not found; run the dataset YAML cell.'\n",
    "\n",
    "model_name = 'yolov8n-pose.pt'  # nano for 4GB VRAM\n",
    "model = YOLO(model_name)\n",
    "\n",
    "# Training options tuned for 4GB VRAM\n",
    "train_args = dict(\n",
    "    data=str(yaml_path),\n",
    "    imgsz=512,       # safer default\n",
    "    epochs=50,       # adjust as needed\n",
    "    batch=4,         # safer default for VRAM\n",
    "    device=0 if torch.cuda.is_available() else 'cpu',\n",
    "    workers=2,       # keep low for laptops\n",
    "    project='runs',\n",
    "    name='pose_plate',\n",
    "    exist_ok=True,\n",
    "    pretrained=True,\n",
    "    cache=False,\n",
    ")\n",
    "\n",
    "results = model.train(**train_args)\n",
    "print('Training done. Best:', results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f539c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Validate & quick inference\n",
    "from glob import glob\n",
    "import os, yaml\n",
    "import cv2, numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# locate last run more robustly\n",
    "pose_runs = sorted(glob('runs/pose/*')) if os.path.exists('runs/pose') else []\n",
    "run_dir = pose_runs[-1] if pose_runs else None\n",
    "print('Last run:', run_dir)\n",
    "\n",
    "# Load model from best/last if available, otherwise prefer local pose weights\n",
    "ckpt = None\n",
    "if run_dir:\n",
    "    for w in ['weights/best.pt', 'weights/last.pt']:\n",
    "        p = os.path.join(run_dir, w)\n",
    "        if os.path.exists(p):\n",
    "            ckpt = p\n",
    "            break\n",
    "\n",
    "# prefer pose weights explicitly to avoid defaulting to plain detection weights\n",
    "pose_candidates = ['yolo11n-pose.pt', 'yolov8n-pose.pt']\n",
    "fallback = next((p for p in pose_candidates if os.path.exists(p)), None) or 'yolov8n-pose.pt'\n",
    "print('Loading weights:', ckpt or fallback)\n",
    "model = YOLO(ckpt or fallback)\n",
    "\n",
    "# read data.yaml to find val images path\n",
    "with open('./dataset/data.yaml', 'r') as f:\n",
    "    data_cfg = yaml.safe_load(f)\n",
    "val_path = data_cfg.get('val')\n",
    "base_path = data_cfg.get('path', '.')\n",
    "val_dir = os.path.join(base_path, val_path) if val_path else './dataset/val/images'\n",
    "\n",
    "# pick a few images\n",
    "val_imgs = []\n",
    "for pat in ['*.jpg', '*.jpeg', '*.png', '*.*']:\n",
    "    val_imgs = glob(os.path.join(val_dir, pat))\n",
    "    if val_imgs:\n",
    "        break\n",
    "val_imgs = val_imgs[:6]\n",
    "print('Previewing', len(val_imgs), 'images from', val_dir)\n",
    "\n",
    "for imgp in val_imgs:\n",
    "    im = cv2.imread(imgp)\n",
    "    res = model.predict(source=im, imgsz=640, conf=0.25, verbose=False)[0]\n",
    "\n",
    "    # Draw boxes\n",
    "    if res.boxes is not None and len(res.boxes) > 0:\n",
    "        for b in res.boxes:\n",
    "            x1,y1,x2,y2 = b.xyxy[0].int().cpu().tolist()\n",
    "            cv2.rectangle(im, (x1,y1), (x2,y2), (0,255,0), 2)\n",
    "\n",
    "    # Draw keypoints\n",
    "    if res.keypoints is not None and res.keypoints.xy is not None:\n",
    "        kxy = res.keypoints.xy\n",
    "        for i in range(kxy.shape[0]):\n",
    "            for k in range(kxy.shape[1]):\n",
    "                x,y = map(int, kxy[i,k])\n",
    "                cv2.circle(im, (x,y), 3, (0,255,255), -1)\n",
    "\n",
    "    cv2.imshow('preview', im)\n",
    "    if cv2.waitKey(0) & 0xFF == ord('q'):\n",
    "        break\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
