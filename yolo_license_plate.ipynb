{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9653af5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4574 label files under 'labels' folders\n",
      "Inferred keypoints K (majority) = 4\n",
      "Images dirs (raw): {'train': 'dataset/images/train', 'val': 'dataset/images/val', 'test': None}\n",
      "Images dirs (normalized): {'train': 'images/train', 'val': 'images/val', 'test': None}\n",
      "Wrote dataset/data.yaml\n",
      "path: /home/azazel/creation/license_plate_recognition/dataset\n",
      "train: images/train\n",
      "val: images/val\n",
      "test: null\n",
      "names:\n",
      "- plate\n",
      "kpt_shape:\n",
      "- 4\n",
      "- 3\n",
      "skeleton:\n",
      "- - 0\n",
      "  - 1\n",
      "- - 1\n",
      "  - 2\n",
      "- - 2\n",
      "  - 3\n",
      "- - 3\n",
      "  - 0\n",
      "flip_idx:\n",
      "- 0\n",
      "- 1\n",
      "- 2\n",
      "- 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2) Dataset scan + build data.yaml for pose (robust scan)\n",
    "from pathlib import Path\n",
    "import os, yaml\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "\n",
    "root = Path('./dataset')\n",
    "assert root.exists(), \"Dataset folder './dataset' not found. Please scp/rsync it first.\"\n",
    "\n",
    "# Find YOLO label .txt anywhere under labels/ (supports both layouts)\n",
    "label_txts = []\n",
    "label_txts += glob(str(root / 'labels' / '**' / '*.txt'), recursive=True)          # type-first: dataset/labels/train/*.txt\n",
    "label_txts += glob(str(root / '*' / 'labels' / '**' / '*.txt'), recursive=True)    # split-first: dataset/train/labels/*.txt\n",
    "label_txts = sorted(set(label_txts))\n",
    "print(f\"Found {len(label_txts)} label files under 'labels' folders\")\n",
    "\n",
    "if len(label_txts) == 0:\n",
    "    # Last resort: search any .txt and warn\n",
    "    label_txts = glob(str(root / '**' / '*.txt'), recursive=True)\n",
    "    print(f\"Fallback: found {len(label_txts)} .txt files total\")\n",
    "    assert len(label_txts)>0, \"No label .txt files found anywhere in './dataset'. Expected YOLO-pose labels under .../labels/...\"\n",
    "\n",
    "# Robustly infer keypoints K by majority across many labels\n",
    "kp_counter = Counter()\n",
    "for i, fp in enumerate(label_txts[:2000]):  # sample up to 2000 files\n",
    "    try:\n",
    "        with open(fp, 'r') as f:\n",
    "            for j, ln in enumerate(f):\n",
    "                ln = ln.strip()\n",
    "                if not ln:\n",
    "                    continue\n",
    "                parts = ln.split()\n",
    "                # prefer full pose labels: cls cx cy w h + 3*K\n",
    "                if len(parts) >= 8 and (len(parts) - 5) % 3 == 0:\n",
    "                    Kcand = (len(parts) - 5) // 3\n",
    "                    kp_counter[Kcand] += 1\n",
    "                # stop early per file after a couple of lines\n",
    "                if j >= 2:\n",
    "                    break\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "if not kp_counter:\n",
    "    # Fallback: detect K from kpt-only (cls x1 y1 x2 y2 ...)\n",
    "    kpt_only_counter = Counter()\n",
    "    for i, fp in enumerate(label_txts[:2000]):\n",
    "        try:\n",
    "            with open(fp, 'r') as f:\n",
    "                for j, ln in enumerate(f):\n",
    "                    ln = ln.strip()\n",
    "                    if not ln:\n",
    "                        continue\n",
    "                    parts = ln.split()\n",
    "                    if len(parts) >= 3 and (len(parts) - 1) % 2 == 0:\n",
    "                        Kcand = (len(parts) - 1) // 2\n",
    "                        kpt_only_counter[Kcand] += 1\n",
    "                    if j >= 2:\n",
    "                        break\n",
    "        except Exception:\n",
    "            continue\n",
    "    assert kpt_only_counter, 'Could not infer keypoints K from labels.'\n",
    "    K = max(kpt_only_counter, key=kpt_only_counter.get)\n",
    "else:\n",
    "    K = max(kp_counter, key=kp_counter.get)\n",
    "\n",
    "print(f\"Inferred keypoints K (majority) = {K}\")\n",
    "\n",
    "# Heuristics to resolve images dirs for each split\n",
    "def choose_images_dir(split: str):\n",
    "    candidates = [\n",
    "        root / 'images' / split,       # type-first\n",
    "        root / split / 'images',       # split-first\n",
    "    ]\n",
    "    # derive from labels/<split>\n",
    "    split_label_dirs = []\n",
    "    for p in label_txts:\n",
    "        p_norm = p.replace('\\\\', '/').lower()\n",
    "        if f\"/labels/{split}/\" in p_norm:\n",
    "            split_label_dirs.append(Path(p).parent)\n",
    "    if split_label_dirs:\n",
    "        ldir = split_label_dirs[0]\n",
    "        candidates += [\n",
    "            ldir.parent / 'images',                  # .../train/images\n",
    "            ldir.parent.parent / 'images' / ldir.name # .../images/train\n",
    "        ]\n",
    "    for c in candidates:\n",
    "        if c.exists():\n",
    "            return str(c)\n",
    "    return None\n",
    "\n",
    "train_path = choose_images_dir('train')\n",
    "val_path = choose_images_dir('val')\n",
    "test_path = choose_images_dir('test')\n",
    "print('Images dirs (raw):', {'train': train_path, 'val': val_path, 'test': test_path})\n",
    "\n",
    "# Normalize to be RELATIVE to dataset root if possible, else absolute\n",
    "root_abs = root.resolve()\n",
    "\n",
    "def to_rel_or_abs(p):\n",
    "    if not p:\n",
    "        return None\n",
    "    p_abs = Path(p)\n",
    "    try:\n",
    "        p_abs = p_abs.resolve()\n",
    "    except Exception:\n",
    "        p_abs = (root / p).resolve()\n",
    "    try:\n",
    "        return str(p_abs.relative_to(root_abs))\n",
    "    except ValueError:\n",
    "        return str(p_abs)\n",
    "\n",
    "train_rel = to_rel_or_abs(train_path)\n",
    "val_rel = to_rel_or_abs(val_path)\n",
    "test_rel = to_rel_or_abs(test_path)\n",
    "print('Images dirs (normalized):', {'train': train_rel, 'val': val_rel, 'test': test_rel})\n",
    "\n",
    "assert train_rel and val_rel, (\n",
    "    \"Could not locate images/train and/or images/val. Ensure YOLO structure like:\\n\"\n",
    "    \"- dataset/images/train & dataset/labels/train\\n\"\n",
    "    \"- dataset/train/images & dataset/train/labels\\n\"\n",
    ")\n",
    "\n",
    "# Build YAML for pose\n",
    "names = ['plate']\n",
    "skeleton = [[0,1],[1,2],[2,3],[3,0]] if K==4 else []\n",
    "flip_idx = list(range(K))\n",
    "\n",
    "data = {\n",
    "    'path': str(root_abs),\n",
    "    'train': train_rel,\n",
    "    'val': val_rel,\n",
    "    'test': test_rel,\n",
    "    'names': names,\n",
    "    'kpt_shape': [K, 3],\n",
    "    'skeleton': skeleton,\n",
    "    'flip_idx': flip_idx,\n",
    "}\n",
    "\n",
    "yaml_path = root/'data.yaml'\n",
    "with open(yaml_path, 'w') as f:\n",
    "    yaml.safe_dump(data, f, sort_keys=False)\n",
    "\n",
    "print('Wrote', yaml_path)\n",
    "print(yaml.safe_dump(data, sort_keys=False))\n",
    "\n",
    "# Clear label caches so Ultralytics rebuilds with the corrected kpt_shape\n",
    "for cache_name in ['train.cache', 'val.cache']:\n",
    "    cp = root / 'labels' / cache_name\n",
    "    if cp.exists():\n",
    "        try:\n",
    "            os.remove(cp)\n",
    "            print('Removed cache:', cp)\n",
    "        except Exception as e:\n",
    "            print('Failed to remove cache', cp, e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d29a507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling files: 10\n",
      "Found non-compliant examples (up to 10 shown):\n",
      "Needs conversion: False\n",
      "Labels appear to already be in YOLOv8-pose format for sampled files. No conversion applied.\n"
     ]
    }
   ],
   "source": [
    "# 2.5) Quick audit (10 files) + auto-fix labels to YOLOv8-Pose format if needed\n",
    "from pathlib import Path\n",
    "import os, shutil, glob\n",
    "\n",
    "labels_root = Path('./dataset/labels')\n",
    "assert labels_root.exists(), 'labels folder not found'\n",
    "\n",
    "# Sample ~10 label files from val to inspect\n",
    "samples = []\n",
    "for split in ['val', 'train']:\n",
    "    cand = sorted(glob.glob(str(labels_root / split / '*.txt')))\n",
    "    if cand:\n",
    "        samples += cand[:max(0, 10 - len(samples))]\n",
    "    if len(samples) >= 10:\n",
    "        break\n",
    "\n",
    "print('Sampling files:', len(samples))\n",
    "\n",
    "# Inspect helper\n",
    "def classify_format(tokens):\n",
    "    # tokens: list[str]\n",
    "    n = len(tokens)\n",
    "    if n >= 6 and (n - 5) % 3 == 0:\n",
    "        return 'yolo_pose_full'  # cls x y w h x1 y1 v1 ...\n",
    "    if n >= 3 and (n - 1) % 2 == 0:\n",
    "        return 'kpt_only'        # cls x1 y1 x2 y2 ... (no bbox, no v)\n",
    "    return 'unknown'\n",
    "\n",
    "bad_examples = []\n",
    "for fp in samples:\n",
    "    with open(fp, 'r') as f:\n",
    "        lines = [ln.strip() for ln in f.readlines() if ln.strip()]\n",
    "    for ln in lines[:2]:  # show at most 2 lines/file\n",
    "        toks = ln.split()\n",
    "        fmt = classify_format(toks)\n",
    "        if fmt != 'yolo_pose_full':\n",
    "            bad_examples.append((fp, ln, fmt))\n",
    "        else:\n",
    "            # quick sanity: visibility values should be integers 0/1/2\n",
    "            try:\n",
    "                # v1 begins at index 7 (0-based): [cls,cx,cy,w,h,x1,y1,v1,x2,y2,v2,...]\n",
    "                vis_vals = [float(v) for v in toks[7::3]]\n",
    "                if not all(v in (0, 1, 2) for v in vis_vals):\n",
    "                    bad_examples.append((fp, ln, 'invalid_visibility'))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "print('Found non-compliant examples (up to 10 shown):')\n",
    "for i, (fp, ln, fmt) in enumerate(bad_examples[:10], 1):\n",
    "    print(f'{i:02d}. {fp} | fmt={fmt} | line=\"{ln[:120]}\"')\n",
    "\n",
    "# If most samples are kpt_only, auto-convert the whole dataset in-place (with backup)\n",
    "needs_convert = any(fmt == 'kpt_only' for _, _, fmt in bad_examples) and not any(fmt == 'yolo_pose_full' for _, _, fmt in bad_examples)\n",
    "print('Needs conversion:', needs_convert)\n",
    "\n",
    "\n",
    "def convert_file_in_place(txt_path: Path, backup_dir: Path, expected_k: int | None = None) -> tuple[int, int]:\n",
    "    \"\"\"Convert kpt-only lines to YOLOv8-pose full format. Returns (ok_lines, total_lines).\"\"\"\n",
    "    with open(txt_path, 'r') as f:\n",
    "        lines = [ln.strip() for ln in f.readlines() if ln.strip()]\n",
    "\n",
    "    out_lines = []\n",
    "    ok, total = 0, 0\n",
    "    for ln in lines:\n",
    "        total += 1\n",
    "        toks = ln.split()\n",
    "        fmt = classify_format(toks)\n",
    "        if fmt == 'yolo_pose_full':\n",
    "            out_lines.append(ln)\n",
    "            ok += 1\n",
    "            continue\n",
    "        if fmt != 'kpt_only':\n",
    "            # skip malformed lines\n",
    "            continue\n",
    "        try:\n",
    "            cls = int(float(toks[0]))\n",
    "        except Exception:\n",
    "            continue\n",
    "        vals = list(map(float, toks[1:]))\n",
    "        if len(vals) % 2 != 0:\n",
    "            continue\n",
    "        K = len(vals) // 2\n",
    "        if expected_k is not None and K != expected_k:\n",
    "            # inconsistent K\n",
    "            continue\n",
    "        xs = vals[0::2]\n",
    "        ys = vals[1::2]\n",
    "        # clamp to [0,1]\n",
    "        xs = [min(1.0, max(0.0, x)) for x in xs]\n",
    "        ys = [min(1.0, max(0.0, y)) for y in ys]\n",
    "        x1, x2 = min(xs), max(xs)\n",
    "        y1, y2 = min(ys), max(ys)\n",
    "        w = max(x2 - x1, 1e-6)\n",
    "        h = max(y2 - y1, 1e-6)\n",
    "        cx = (x1 + x2) / 2.0\n",
    "        cy = (y1 + y2) / 2.0\n",
    "        # visibility: mark as 2 (labeled & visible)\n",
    "        kv = []\n",
    "        for i in range(K):\n",
    "            kv += [xs[i], ys[i], 2]\n",
    "        new_toks = [str(cls), f'{cx:.6f}', f'{cy:.6f}', f'{w:.6f}', f'{h:.6f}'] + [f'{v:.6f}' if isinstance(v, float) else str(v) for v in kv]\n",
    "        out_lines.append(' '.join(new_toks))\n",
    "        ok += 1\n",
    "\n",
    "    if ok:\n",
    "        # backup original once\n",
    "        backup_path = backup_dir / txt_path.name\n",
    "        if not backup_path.exists():\n",
    "            shutil.copy2(txt_path, backup_path)\n",
    "        with open(txt_path, 'w') as f:\n",
    "            f.write('\\n'.join(out_lines) + ('\\n' if out_lines else ''))\n",
    "    return ok, total\n",
    "\n",
    "if needs_convert:\n",
    "    backup_root = labels_root.parent / 'labels_backup_before_pose_fix'\n",
    "    backup_root.mkdir(exist_ok=True)\n",
    "    sum_ok = 0\n",
    "    sum_total = 0\n",
    "    # Infer K from first bad sample\n",
    "    K_guess = None\n",
    "    for _, ln, fmt in bad_examples:\n",
    "        if fmt == 'kpt_only':\n",
    "            toks = ln.split()\n",
    "            K_guess = (len(toks) - 1) // 2\n",
    "            break\n",
    "    print('Assumed K from samples =', K_guess)\n",
    "\n",
    "    for split in ['train', 'val']:\n",
    "        split_dir = labels_root / split\n",
    "        if not split_dir.exists():\n",
    "            continue\n",
    "        backup_dir = backup_root / split\n",
    "        backup_dir.mkdir(parents=True, exist_ok=True)\n",
    "        files = sorted(split_dir.glob('*.txt'))\n",
    "        for i, fp in enumerate(files, 1):\n",
    "            ok, total = convert_file_in_place(fp, backup_dir, expected_k=K_guess)\n",
    "            sum_ok += ok\n",
    "            sum_total += total\n",
    "            if i % 500 == 0:\n",
    "                print(f'Converted {i}/{len(files)} files...')\n",
    "    print(f'Converted lines: {sum_ok}/{sum_total}. Backup at: {backup_root}')\n",
    "\n",
    "    # Clear caches so Ultralytics rebuilds with new labels\n",
    "    for cache_name in ['train.cache', 'val.cache']:\n",
    "        cp = labels_root / cache_name\n",
    "        if cp.exists():\n",
    "            try:\n",
    "                os.remove(cp)\n",
    "                print('Removed cache:', cp)\n",
    "            except Exception as e:\n",
    "                print('Failed to remove cache', cp, e)\n",
    "else:\n",
    "    print('Labels appear to already be in YOLOv8-pose format for sampled files. No conversion applied.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bb3c0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized class IDs in 0 files, 0 lines changed to class 0.\n"
     ]
    }
   ],
   "source": [
    "# 2.6) Normalize class IDs to single class (0) and clear caches\n",
    "from pathlib import Path\n",
    "import os, glob\n",
    "\n",
    "labels_root = Path('./dataset/labels')\n",
    "assert labels_root.exists(), 'labels folder not found'\n",
    "\n",
    "changed_lines = 0\n",
    "changed_files = 0\n",
    "\n",
    "def normalize_classes(txt_path: Path) -> tuple[int, bool]:\n",
    "    with open(txt_path, 'r') as f:\n",
    "        lines = [ln.rstrip('\\n') for ln in f.readlines()]\n",
    "    out = []\n",
    "    changed = False\n",
    "    count = 0\n",
    "    for ln in lines:\n",
    "        if not ln.strip():\n",
    "            out.append(ln)\n",
    "            continue\n",
    "        toks = ln.split()\n",
    "        try:\n",
    "            cls = int(float(toks[0]))\n",
    "        except Exception:\n",
    "            out.append(ln)\n",
    "            continue\n",
    "        if cls != 0:\n",
    "            toks[0] = '0'\n",
    "            changed = True\n",
    "            count += 1\n",
    "        out.append(' '.join(toks))\n",
    "    if changed:\n",
    "        with open(txt_path, 'w') as f:\n",
    "            f.write('\\n'.join(out) + ('\\n' if out and out[-1] != '' else ''))\n",
    "    return count, changed\n",
    "\n",
    "for split in ['train', 'val']:\n",
    "    files = sorted((labels_root / split).glob('*.txt')) if (labels_root / split).exists() else []\n",
    "    for fp in files:\n",
    "        cnt, ch = normalize_classes(fp)\n",
    "        changed_lines += cnt\n",
    "        changed_files += int(ch)\n",
    "\n",
    "print(f'Normalized class IDs in {changed_files} files, {changed_lines} lines changed to class 0.')\n",
    "\n",
    "# Clear caches again\n",
    "for cache_name in ['train.cache', 'val.cache']:\n",
    "    cp = labels_root / cache_name\n",
    "    if cp.exists():\n",
    "        try:\n",
    "            os.remove(cp)\n",
    "            print('Removed cache:', cp)\n",
    "        except Exception as e:\n",
    "            print('Failed to remove cache', cp, e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7c2501e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.194 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.192 🚀 Python-3.13.7 torch-2.8.0+cu128 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 3768MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=4, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=dataset/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=512, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n-pose.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=pose_plate, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/pose_plate, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=pose, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=2, workspace=None\n",
      "Overriding model.yaml kpt_shape=[17, 3] with kpt_shape=[4, 3]\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "Ultralytics 8.3.192 🚀 Python-3.13.7 torch-2.8.0+cu128 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 3768MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=4, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=dataset/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=512, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n-pose.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=pose_plate, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/pose_plate, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=pose, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=2, workspace=None\n",
      "Overriding model.yaml kpt_shape=[17, 3] with kpt_shape=[4, 3]\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    823735  ultralytics.nn.modules.head.Pose             [1, [4, 3], [64, 128, 256]]   \n",
      " 22        [15, 18, 21]  1    823735  ultralytics.nn.modules.head.Pose             [1, [4, 3], [64, 128, 256]]   \n",
      "YOLOv8n-pose summary: 144 layers, 3,083,271 parameters, 3,083,255 gradients, 8.4 GFLOPs\n",
      "\n",
      "YOLOv8n-pose summary: 144 layers, 3,083,271 parameters, 3,083,255 gradients, 8.4 GFLOPs\n",
      "\n",
      "Transferred 361/397 items from pretrained weights\n",
      "Transferred 361/397 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% ━━━━━━━━━━━━ 5.4/5.4MB 9.1MB/s 0.6s<0.1s\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% ━━━━━━━━━━━━ 5.4/5.4MB 9.1MB/s 0.6s\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 3967.2±1527.6 MB/s, size: 179.8 KB)\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 3967.2±1527.6 MB/s, size: 179.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/azazel/creation/license_plate_recognition/dataset/labels/train... 3429 images, 1 backgrounds, 15 corrupt: 100% ━━━━━━━━━━━━ 3430/3430 2231.2it/s 1.5s0.1s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/train/Dieu_0024.png: ignoring corrupt image/label: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/train/Dieu_0420.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/train/Hung_0102.png: ignoring corrupt image/label: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/train/Hung_0158.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/train/Hung_0169.png: ignoring corrupt image/label: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/train/Hung_0175.png: ignoring corrupt image/label: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (9,) + inhomogeneous part.\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/train/Hung_0179.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/train/Hung_0198.png: ignoring corrupt image/label: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/train/Tgmt_0015.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/train/Tgmt_0030.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/train/Tgmt_0036.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/train/Tgmt_0039.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/train/Tgmt_0236.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/train/Tgmt_0776.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/train/greenpack_1342.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/azazel/creation/license_plate_recognition/dataset/labels/train... 3429 images, 1 backgrounds, 15 corrupt: 100% ━━━━━━━━━━━━ 3430/3430 2231.2it/s 1.5s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/train/Dieu_0024.png: ignoring corrupt image/label: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/train/Dieu_0420.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/train/Hung_0102.png: ignoring corrupt image/label: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/train/Hung_0158.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/train/Hung_0169.png: ignoring corrupt image/label: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/train/Hung_0175.png: ignoring corrupt image/label: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (9,) + inhomogeneous part.\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/train/Hung_0179.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/train/Hung_0198.png: ignoring corrupt image/label: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/train/Tgmt_0015.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/train/Tgmt_0030.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/train/Tgmt_0036.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/train/Tgmt_0039.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/train/Tgmt_0236.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/train/Tgmt_0776.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/train/greenpack_1342.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/azazel/creation/license_plate_recognition/dataset/labels/train.cache\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/azazel/creation/license_plate_recognition/dataset/labels/train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1317.6±910.9 MB/s, size: 121.2 KB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1317.6±910.9 MB/s, size: 121.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/azazel/creation/license_plate_recognition/dataset/labels/val... 1145 images, 0 backgrounds, 17 corrupt: 100% ━━━━━━━━━━━━ 1145/1145 2127.0it/s 0.5s1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/Dieu_0103.png: ignoring corrupt image/label: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (4,) + inhomogeneous part.\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/Hung_0156.png: ignoring corrupt image/label: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/Hung_0157.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/Hung_0178.png: ignoring corrupt image/label: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (4,) + inhomogeneous part.\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/Hung_0195.png: ignoring corrupt image/label: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/Hung_0200.png: ignoring corrupt image/label: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/Tgmt_0016.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/Tgmt_0084.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/Tgmt_0145.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/Tgmt_0146.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/Tgmt_0155.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/Tgmt_0285.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/Tgmt_0305.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/Tgmt_0791.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/greenpack_1341.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/greenpack_1345.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/greenpack_1346.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/azazel/creation/license_plate_recognition/dataset/labels/val... 1145 images, 0 backgrounds, 17 corrupt: 100% ━━━━━━━━━━━━ 1145/1145 2127.0it/s 0.5s\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/Dieu_0103.png: ignoring corrupt image/label: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (4,) + inhomogeneous part.\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/Hung_0156.png: ignoring corrupt image/label: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/Hung_0157.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/Hung_0178.png: ignoring corrupt image/label: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (4,) + inhomogeneous part.\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/Hung_0195.png: ignoring corrupt image/label: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/Hung_0200.png: ignoring corrupt image/label: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/Tgmt_0016.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/Tgmt_0084.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/Tgmt_0145.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/Tgmt_0146.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/Tgmt_0155.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/Tgmt_0285.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/Tgmt_0305.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/Tgmt_0791.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/greenpack_1341.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/greenpack_1345.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/azazel/creation/license_plate_recognition/dataset/images/val/greenpack_1346.png: ignoring corrupt image/label: labels require 17 columns each\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/azazel/creation/license_plate_recognition/dataset/labels/val.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/azazel/creation/license_plate_recognition/dataset/labels/val.cache\n",
      "Plotting labels to runs/pose_plate/labels.jpg... \n",
      "Plotting labels to runs/pose_plate/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 63 weight(decay=0.0), 73 weight(decay=0.0005), 72 bias(decay=0.0)\n",
      "Image sizes 512 train, 512 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns/pose_plate\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 63 weight(decay=0.0), 73 weight(decay=0.0005), 72 bias(decay=0.0)\n",
      "Image sizes 512 train, 512 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns/pose_plate\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/50     0.447G      1.116      5.618     0.5813      1.085      1.168          8        512: 100% ━━━━━━━━━━━━ 854/854 15.6it/s 54.6s<0.1s\n",
      "\u001b[K       1/50     0.447G      1.116      5.618     0.5813      1.085      1.168          8        512: 100% ━━━━━━━━━━━━ 854/854 15.6it/s 54.6s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 141/141 20.9it/s 6.7s0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 141/141 20.9it/s 6.7s\n",
      "                   all       1128       1285      0.926      0.879      0.944      0.688      0.687      0.637       0.51      0.299\n",
      "                   all       1128       1285      0.926      0.879      0.944      0.688      0.687      0.637       0.51      0.299\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/50     0.465G     0.9836       2.83     0.4639     0.7608      1.047          8        512: 100% ━━━━━━━━━━━━ 854/854 17.8it/s 47.9s<0.1s\n",
      "\u001b[K       2/50     0.465G     0.9836       2.83     0.4639     0.7608      1.047          8        512: 100% ━━━━━━━━━━━━ 854/854 17.8it/s 47.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 141/141 22.4it/s 6.3s0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 141/141 22.4it/s 6.3s\n",
      "                   all       1128       1285      0.902      0.915      0.963      0.749      0.733      0.658      0.661       0.53\n",
      "                   all       1128       1285      0.902      0.915      0.963      0.749      0.733      0.658      0.661       0.53\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/50      0.48G      0.968      2.399     0.4493     0.7474      1.041          5        512: 100% ━━━━━━━━━━━━ 854/854 18.4it/s 46.4s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 141/141 21.7it/s 6.5s0.1s\n",
      "                   all       1128       1285      0.937      0.897      0.959      0.741      0.851      0.785      0.774      0.677\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/50       0.5G     0.9414      2.138     0.4365     0.7189      1.036          6        512: 100% ━━━━━━━━━━━━ 854/854 18.0it/s 47.4s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 141/141 21.9it/s 6.4s0.0s\n",
      "                   all       1128       1285       0.95      0.881      0.955      0.754      0.849      0.745      0.759      0.683\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/50     0.518G     0.9289      1.985     0.4138     0.6775      1.023          7        512: 100% ━━━━━━━━━━━━ 854/854 18.2it/s 47.0s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 141/141 22.5it/s 6.3s0.0s\n",
      "                   all       1128       1285      0.962      0.925      0.971      0.748      0.864      0.817      0.799      0.725\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/50     0.535G     0.8769      1.747     0.3833     0.6369      1.006          4        512: 100% ━━━━━━━━━━━━ 854/854 17.7it/s 48.3s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 141/141 22.0it/s 6.4s0.1s\n",
      "                   all       1128       1285      0.926      0.943      0.975      0.788      0.903      0.807      0.859      0.813\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/50     0.553G     0.8483      1.718     0.3721     0.6064     0.9973          9        512: 100% ━━━━━━━━━━━━ 854/854 17.9it/s 47.6s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 141/141 21.8it/s 6.5s0.0s\n",
      "                   all       1128       1285      0.966      0.949      0.979      0.788      0.891      0.845      0.841      0.788\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/50      0.57G     0.8282      1.676     0.3534     0.5889     0.9909          6        512: 100% ━━━━━━━━━━━━ 854/854 17.5it/s 48.7s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 141/141 21.7it/s 6.5s0.0s\n",
      "                   all       1128       1285      0.956      0.941      0.983      0.803      0.856      0.826      0.818      0.778\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/50     0.588G     0.8071      1.589     0.3416     0.5767     0.9787          4        512: 100% ━━━━━━━━━━━━ 854/854 18.0it/s 47.5s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 141/141 21.4it/s 6.6s0.1s\n",
      "                   all       1128       1285      0.938       0.95      0.981      0.804      0.851      0.861      0.831        0.8\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/50     0.605G     0.8282      1.567     0.3182     0.5795     0.9858          7        512: 100% ━━━━━━━━━━━━ 854/854 17.5it/s 48.7s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 141/141 20.8it/s 6.8s0.1s\n",
      "                   all       1128       1285      0.969      0.947       0.98      0.814      0.888      0.859      0.836      0.803\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      11/50     0.621G     0.7878      1.461     0.3093     0.5469     0.9689          5        512: 100% ━━━━━━━━━━━━ 854/854 17.6it/s 48.4s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 141/141 21.5it/s 6.6s0.1s\n",
      "                   all       1128       1285      0.963      0.963      0.986      0.809      0.883      0.876       0.85      0.817\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      12/50     0.641G     0.7778      1.444     0.3034     0.5402     0.9685          4        512: 100% ━━━━━━━━━━━━ 854/854 17.7it/s 48.1s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 141/141 21.9it/s 6.4s0.0s\n",
      "                   all       1128       1285      0.972      0.957      0.988      0.832      0.883      0.849      0.833      0.808\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      13/50     0.656G     0.7703      1.386     0.2933     0.5377     0.9669          7        512: 100% ━━━━━━━━━━━━ 854/854 18.1it/s 47.3s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 141/141 22.1it/s 6.4s0.0s\n",
      "                   all       1128       1285      0.981      0.957      0.987      0.812      0.897      0.873      0.849      0.818\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      14/50     0.676G     0.7655      1.328     0.2806      0.527     0.9634          7        512: 100% ━━━━━━━━━━━━ 854/854 17.7it/s 48.2s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 141/141 22.1it/s 6.4s0.0s\n",
      "                   all       1128       1285      0.983      0.959      0.989       0.84      0.862      0.825      0.785      0.767\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      15/50     0.693G     0.7546      1.331     0.2724     0.5058     0.9548          7        512: 100% ━━━━━━━━━━━━ 854/854 17.2it/s 49.5s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 141/141 20.0it/s 7.0s0.0s\n",
      "                   all       1128       1285      0.975      0.958      0.987      0.831      0.893      0.873      0.851      0.816\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      16/50     0.711G     0.7438      1.266     0.2628     0.5041     0.9524          9        512: 100% ━━━━━━━━━━━━ 854/854 17.7it/s 48.3s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 141/141 21.9it/s 6.4s0.0s\n",
      "                   all       1128       1285      0.979      0.959      0.985      0.828      0.904      0.879      0.872      0.846\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      17/50     0.729G     0.7396      1.252     0.2543     0.4923     0.9518          6        512: 100% ━━━━━━━━━━━━ 854/854 17.5it/s 48.8s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 141/141 20.6it/s 6.9s0.1s\n",
      "                   all       1128       1285      0.976      0.968       0.99      0.833      0.871      0.849      0.809      0.784\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      18/50     0.744G     0.7241      1.197     0.2476     0.4889     0.9466          7        512: 100% ━━━━━━━━━━━━ 854/854 16.1it/s 52.9s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 141/141 19.9it/s 7.1s0.1s\n",
      "                   all       1128       1285      0.976      0.959      0.989      0.843      0.887      0.867      0.844      0.823\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      19/50     0.762G     0.7096      1.187     0.2362     0.4774     0.9408          7        512: 100% ━━━━━━━━━━━━ 854/854 16.8it/s 50.7s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 141/141 19.7it/s 7.2s0.1s\n",
      "                   all       1128       1285      0.981      0.966      0.986      0.851      0.877      0.852      0.829      0.808\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      20/50     0.779G     0.7027      1.106     0.2309      0.469     0.9418          5        512: 100% ━━━━━━━━━━━━ 854/854 17.5it/s 48.8s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 141/141 22.2it/s 6.4s0.1s\n",
      "                   all       1128       1285      0.973       0.96      0.991      0.849      0.889      0.862      0.838       0.82\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      21/50     0.797G     0.6863      1.057      0.214     0.4573     0.9288          6        512: 100% ━━━━━━━━━━━━ 854/854 18.0it/s 47.4s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 141/141 20.3it/s 6.9s0.1s\n",
      "                   all       1128       1285      0.983      0.971      0.992      0.845      0.887      0.871      0.829      0.809\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      22/50     0.814G     0.6999      1.083     0.2198     0.4613     0.9377          6        512: 100% ━━━━━━━━━━━━ 854/854 16.9it/s 50.5s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 141/141 19.2it/s 7.4s0.1s\n",
      "                   all       1128       1285      0.974      0.973      0.989      0.845      0.882      0.881      0.854      0.836\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      23/50     0.834G      0.681      1.005     0.2137     0.4555     0.9313          7        512: 100% ━━━━━━━━━━━━ 854/854 15.4it/s 55.5s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 141/141 18.5it/s 7.6s0.1s\n",
      "                   all       1128       1285      0.976      0.973       0.99      0.849      0.883       0.87      0.826      0.811\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      24/50     0.852G     0.6912      1.028     0.2054     0.4512     0.9322          6        512: 100% ━━━━━━━━━━━━ 854/854 17.0it/s 50.4s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 141/141 21.1it/s 6.7s0.1s\n",
      "                   all       1128       1285      0.971      0.973      0.991      0.859      0.871      0.858      0.829      0.813\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      25/50     0.867G     0.6839      1.024     0.2034     0.4539      0.927          5        512: 100% ━━━━━━━━━━━━ 854/854 16.2it/s 52.8s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 141/141 18.6it/s 7.6s0.1s\n",
      "                   all       1128       1285      0.987       0.97      0.992      0.861      0.897       0.88      0.841      0.826\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      26/50     0.885G     0.6645      0.967     0.2028     0.4366     0.9261         10        512: 100% ━━━━━━━━━━━━ 854/854 17.0it/s 50.1s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 141/141 22.0it/s 6.4s0.0s\n",
      "                   all       1128       1285      0.987      0.969      0.993      0.854      0.893      0.874      0.842      0.826\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      27/50       0.9G     0.6707     0.9624     0.1931     0.4448       0.93         10        512: 99% ━━━━━━━━━━━╸ 847/854 18.0it/s 50.7s<0.4ss"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-65 (_pin_memory_loop):\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/usr/lib64/python3.13/threading.py\"\u001b[0m, line \u001b[35m1043\u001b[0m, in \u001b[35m_bootstrap_inner\u001b[0m\n",
      "    \u001b[31mself.run\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/azazel/creation/license_plate_recognition/.venv/lib64/python3.13/site-packages/ipykernel/ipkernel.py\"\u001b[0m, line \u001b[35m772\u001b[0m, in \u001b[35mrun_closure\u001b[0m\n",
      "    \u001b[31m_threading_Thread_run\u001b[0m\u001b[1;31m(self)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/usr/lib64/python3.13/threading.py\"\u001b[0m, line \u001b[35m994\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    \u001b[31mself._target\u001b[0m\u001b[1;31m(*self._args, **self._kwargs)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/azazel/creation/license_plate_recognition/.venv/lib64/python3.13/site-packages/torch/utils/data/_utils/pin_memory.py\"\u001b[0m, line \u001b[35m61\u001b[0m, in \u001b[35m_pin_memory_loop\u001b[0m\n",
      "    \u001b[31mdo_one_step\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/azazel/creation/license_plate_recognition/.venv/lib64/python3.13/site-packages/torch/utils/data/_utils/pin_memory.py\"\u001b[0m, line \u001b[35m37\u001b[0m, in \u001b[35mdo_one_step\u001b[0m\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \u001b[35m\"/usr/lib64/python3.13/multiprocessing/queues.py\"\u001b[0m, line \u001b[35m120\u001b[0m, in \u001b[35mget\u001b[0m\n",
      "    return \u001b[31m_ForkingPickler.loads\u001b[0m\u001b[1;31m(res)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/azazel/creation/license_plate_recognition/.venv/lib64/python3.13/site-packages/torch/multiprocessing/reductions.py\"\u001b[0m, line \u001b[35m541\u001b[0m, in \u001b[35mrebuild_storage_fd\u001b[0m\n",
      "    fd = df.detach()\n",
      "  File \u001b[35m\"/usr/lib64/python3.13/multiprocessing/resource_sharer.py\"\u001b[0m, line \u001b[35m57\u001b[0m, in \u001b[35mdetach\u001b[0m\n",
      "    with \u001b[31m_resource_sharer.get_connection\u001b[0m\u001b[1;31m(self._id)\u001b[0m as conn:\n",
      "         \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/usr/lib64/python3.13/multiprocessing/resource_sharer.py\"\u001b[0m, line \u001b[35m86\u001b[0m, in \u001b[35mget_connection\u001b[0m\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \u001b[35m\"/usr/lib64/python3.13/multiprocessing/connection.py\"\u001b[0m, line \u001b[35m519\u001b[0m, in \u001b[35mClient\u001b[0m\n",
      "    c = SocketClient(address)\n",
      "  File \u001b[35m\"/usr/lib64/python3.13/multiprocessing/connection.py\"\u001b[0m, line \u001b[35m647\u001b[0m, in \u001b[35mSocketClient\u001b[0m\n",
      "    \u001b[31ms.connect\u001b[0m\u001b[1;31m(address)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^\u001b[0m\n",
      "\u001b[1;35mFileNotFoundError\u001b[0m: \u001b[35m[Errno 2] No such file or directory\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K      27/50       0.9G     0.6707     0.9624     0.1931     0.4448       0.93         10        512: 99% ━━━━━━━━━━━╸ 848/854 17.4it/s 50.8s<0.3s\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Training options tuned for 4GB VRAM\u001b[39;00m\n\u001b[32m     13\u001b[39m train_args = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m     14\u001b[39m     data=\u001b[38;5;28mstr\u001b[39m(yaml_path),\n\u001b[32m     15\u001b[39m     imgsz=\u001b[32m512\u001b[39m,       \u001b[38;5;66;03m# safer default\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m     cache=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     25\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m results = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtrain_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mTraining done. Best:\u001b[39m\u001b[33m'\u001b[39m, results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/creation/license_plate_recognition/.venv/lib64/python3.13/site-packages/ultralytics/engine/model.py:801\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    798\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m    800\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.hub_session = \u001b[38;5;28mself\u001b[39m.session  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m801\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[32m    803\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/creation/license_plate_recognition/.venv/lib64/python3.13/site-packages/ultralytics/engine/trainer.py:230\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    227\u001b[39m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/creation/license_plate_recognition/.venv/lib64/python3.13/site-packages/ultralytics/engine/trainer.py:416\u001b[39m, in \u001b[36mBaseTrainer._do_train\u001b[39m\u001b[34m(self, world_size)\u001b[39m\n\u001b[32m    411\u001b[39m     \u001b[38;5;28mself\u001b[39m.tloss = (\n\u001b[32m    412\u001b[39m         (\u001b[38;5;28mself\u001b[39m.tloss * i + \u001b[38;5;28mself\u001b[39m.loss_items) / (i + \u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loss_items\n\u001b[32m    413\u001b[39m     )\n\u001b[32m    415\u001b[39m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[38;5;66;03m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ni - last_opt_step >= \u001b[38;5;28mself\u001b[39m.accumulate:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/creation/license_plate_recognition/.venv/lib64/python3.13/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/creation/license_plate_recognition/.venv/lib64/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/creation/license_plate_recognition/.venv/lib64/python3.13/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 3) Train\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "yaml_path = Path('./dataset/data.yaml')\n",
    "assert yaml_path.exists(), 'data.yaml not found; run the dataset YAML cell.'\n",
    "\n",
    "model_name = 'yolov8n-pose.pt'  # nano for 4GB VRAM\n",
    "model = YOLO(model_name)\n",
    "\n",
    "# Training options tuned for 4GB VRAM\n",
    "train_args = dict(\n",
    "    data=str(yaml_path),\n",
    "    imgsz=512,       # safer default\n",
    "    epochs=50,       # adjust as needed\n",
    "    batch=4,         # safer default for VRAM\n",
    "    device=0 if torch.cuda.is_available() else 'cpu',\n",
    "    workers=2,       # keep low for laptops\n",
    "    project='runs',\n",
    "    name='pose_plate',\n",
    "    exist_ok=True,\n",
    "    pretrained=True,\n",
    "    cache=False,\n",
    ")\n",
    "\n",
    "results = model.train(**train_args)\n",
    "print('Training done. Best:', results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f539c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last run: None\n"
     ]
    }
   ],
   "source": [
    "# 4) Validate & quick inference\n",
    "from glob import glob\n",
    "import os, yaml\n",
    "import cv2, numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# locate last run more robustly\n",
    "pose_runs = sorted(glob('runs/pose/*')) if os.path.exists('runs/pose') else []\n",
    "run_dir = pose_runs[-1] if pose_runs else None\n",
    "print('Last run:', run_dir)\n",
    "\n",
    "# Load model from best/last if available, otherwise prefer local pose weights\n",
    "ckpt = None\n",
    "if run_dir:\n",
    "    for w in ['weights/best.pt', 'weights/last.pt']:\n",
    "        p = os.path.join(run_dir, w)\n",
    "        if os.path.exists(p):\n",
    "            ckpt = p\n",
    "            break\n",
    "\n",
    "# prefer pose weights explicitly to avoid defaulting to plain detection weights\n",
    "pose_candidates = ['yolo11n-pose.pt', 'yolov8n-pose.pt']\n",
    "fallback = next((p for p in pose_candidates if os.path.exists(p)), None) or 'yolov8n-pose.pt'\n",
    "print('Loading weights:', ckpt or fallback)\n",
    "model = YOLO(ckpt or fallback)\n",
    "\n",
    "# read data.yaml to find val images path\n",
    "with open('./dataset/data.yaml', 'r') as f:\n",
    "    data_cfg = yaml.safe_load(f)\n",
    "val_path = data_cfg.get('val')\n",
    "base_path = data_cfg.get('path', '.')\n",
    "val_dir = os.path.join(base_path, val_path) if val_path else './dataset/val/images'\n",
    "\n",
    "# pick a few images\n",
    "val_imgs = []\n",
    "for pat in ['*.jpg', '*.jpeg', '*.png', '*.*']:\n",
    "    val_imgs = glob(os.path.join(val_dir, pat))\n",
    "    if val_imgs:\n",
    "        break\n",
    "val_imgs = val_imgs[:6]\n",
    "print('Previewing', len(val_imgs), 'images from', val_dir)\n",
    "\n",
    "for imgp in val_imgs:\n",
    "    im = cv2.imread(imgp)\n",
    "    res = model.predict(source=im, imgsz=640, conf=0.25, verbose=False)[0]\n",
    "\n",
    "    # Draw boxes\n",
    "    if res.boxes is not None and len(res.boxes) > 0:\n",
    "        for b in res.boxes:\n",
    "            x1,y1,x2,y2 = b.xyxy[0].int().cpu().tolist()\n",
    "            cv2.rectangle(im, (x1,y1), (x2,y2), (0,255,0), 2)\n",
    "\n",
    "    # Draw keypoints\n",
    "    if res.keypoints is not None and res.keypoints.xy is not None:\n",
    "        kxy = res.keypoints.xy\n",
    "        for i in range(kxy.shape[0]):\n",
    "            for k in range(kxy.shape[1]):\n",
    "                x,y = map(int, kxy[i,k])\n",
    "                cv2.circle(im, (x,y), 3, (0,255,255), -1)\n",
    "\n",
    "    cv2.imshow('preview', im)\n",
    "    if cv2.waitKey(0) & 0xFF == ord('q'):\n",
    "        break\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
