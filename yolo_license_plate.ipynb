{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343889f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download yolov8n-pose backbone (cached by Ultralytics)\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "\n",
    "target = Path('yolov8n-pose.pt')\n",
    "if target.exists():\n",
    "    print('yolov8n-pose.pt already exists at', target)\n",
    "else:\n",
    "    print('Downloading yolov8n-pose.pt (this will be cached by Ultralytics) ...')\n",
    "    # Loading via YOLO(...) will download and cache the weights if not present\n",
    "    _ = YOLO('yolov8n-pose.pt')\n",
    "    print('Download triggered. The file will be available from Ultralytics cache; if you need a local copy, you can export or copy it from the cache.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c2501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train (local-friendly version)\n",
    "import os, yaml\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Detect Kaggle vs local\n",
    "IS_KAGGLE = bool(os.environ.get('KAGGLE_URL_BASE')) or Path('/kaggle/working').exists()\n",
    "print('Environment:', 'Kaggle' if IS_KAGGLE else 'Local')\n",
    "\n",
    "# Dataset preference order:\n",
    "# 1. merged_dataset/data_merged.yaml (nếu đã merge)\n",
    "# 2. dataset4corner/dataset.yaml\n",
    "# 3. dataset/data.yaml\n",
    "candidates = [\n",
    "    Path('merged_dataset/data_merged.yaml'),\n",
    "    Path('dataset4corner/dataset.yaml'),\n",
    "    Path('dataset/data.yaml'),\n",
    "]\n",
    "source_yaml = next((p for p in candidates if p.exists()), None)\n",
    "assert source_yaml is not None, 'Không tìm thấy bất kỳ YAML dataset nào (merged_dataset/data_merged.yaml | dataset4corner/dataset.yaml | dataset/data.yaml)'\n",
    "print('Using source dataset YAML ->', source_yaml)\n",
    "\n",
    "with open(source_yaml, 'r') as f:\n",
    "    data = yaml.safe_load(f) or {}\n",
    "\n",
    "# --- PATH NORMALIZATION + FALLBACKS ---\n",
    "# Một số YAML (vd dataset4corner/dataset.yaml) có thể chứa đường dẫn tuyệt đối từ máy khác -> gây lỗi.\n",
    "raw_path = data.get('path')\n",
    "print('Raw path from YAML:', raw_path)\n",
    "\n",
    "# Ưu tiên: nếu người dùng set FORCE_DATASET_PATH thì dùng luôn\n",
    "force_path = os.environ.get('FORCE_DATASET_PATH')\n",
    "if force_path:\n",
    "    print('FORCE_DATASET_PATH override ->', force_path)\n",
    "    data['path'] = force_path\n",
    "\n",
    "# Nếu không có override, xử lý logic mặc định\n",
    "if not force_path:\n",
    "    # base ứng viên lấy từ YAML nếu có, ngược lại folder chứa YAML\n",
    "    base_candidate = Path(raw_path) if raw_path else source_yaml.parent\n",
    "\n",
    "    # Nếu base_candidate là absolute nhưng không tồn tại -> fallback\n",
    "    if raw_path and not base_candidate.exists():\n",
    "        print('[WARN] Absolute path trong YAML không tồn tại -> fallback source folder')\n",
    "        base_candidate = source_yaml.parent\n",
    "\n",
    "    # Nếu thư mục ảnh train không tồn tại trong base_candidate -> thử các fallback khác\n",
    "    expected_train = base_candidate / 'images' / 'train'\n",
    "    expected_val   = base_candidate / 'images' / 'val'\n",
    "    if not expected_train.exists() or not expected_val.exists():\n",
    "        print('[WARN] images/train hoặc images/val không tồn tại dưới base_candidate:', base_candidate)\n",
    "        # Thử fallback 1: chính thư mục bên cạnh YAML (source_yaml.parent)\n",
    "        alt1 = source_yaml.parent\n",
    "        if (alt1 / 'images' / 'train').exists():\n",
    "            print(' -> Fallback thành công: dùng', alt1)\n",
    "            base_candidate = alt1\n",
    "        else:\n",
    "            # Thử fallback 2: không đặt path (YOLO sẽ đọc đường dẫn tương đối)\n",
    "            print(' -> Fallback alt1 thất bại. Xoá khóa path để YOLO dùng relative paths.')\n",
    "            data.pop('path', None)\n",
    "            base_candidate = None\n",
    "\n",
    "    if base_candidate is not None:\n",
    "        data['path'] = str(base_candidate.resolve())\n",
    "\n",
    "# Nếu vẫn chưa có path key, bảo đảm train/val là relative\n",
    "if 'path' not in data:\n",
    "    print('[INFO] Sử dụng relative paths (không có khóa path trong YAML output).')\n",
    "\n",
    "# Chuẩn hoá train/val key (nếu YAML cũ dùng image/train ... vẫn giữ nguyên)\n",
    "train_key = data.get('train', 'images/train')\n",
    "val_key   = data.get('val', 'images/val')\n",
    "\n",
    "# Loại bỏ test nếu không rõ\n",
    "if not data.get('test'):\n",
    "    data.pop('test', None)\n",
    "\n",
    "# Đảm bảo các khóa pose\n",
    "data.setdefault('names', ['plate'])\n",
    "data.setdefault('kpt_shape', [4, 3])\n",
    "data.setdefault('skeleton', [[0,1],[1,2],[2,3],[3,0]])\n",
    "data.setdefault('flip_idx', [0,1,2,3])\n",
    "\n",
    "# Ghi lại train/val (phòng trường hợp thay đổi biến)\n",
    "data['train'] = train_key\n",
    "data['val']   = val_key\n",
    "\n",
    "# Đường dẫn YAML xuất ra\n",
    "if IS_KAGGLE:\n",
    "    out_yaml = Path('/kaggle/working/data.yaml')\n",
    "else:\n",
    "    out_yaml = Path('data_local.yaml')  # local file trong project root\n",
    "\n",
    "out_yaml.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(out_yaml, 'w') as f:\n",
    "    yaml.safe_dump(data, f, sort_keys=False)\n",
    "print('Written normalized YAML ->', out_yaml)\n",
    "print('--- Final YAML content ---')\n",
    "print(yaml.safe_dump(data, sort_keys=False))\n",
    "\n",
    "# Sanity check tồn tại thư mục\n",
    "paths_to_check = []\n",
    "base_for_check = Path(data['path']) if 'path' in data else source_yaml.parent\n",
    "paths_to_check.extend([\n",
    "    base_for_check/'images'/'train',\n",
    "    base_for_check/'images'/'val',\n",
    "    base_for_check/'labels'/'train',\n",
    "    base_for_check/'labels'/'val',\n",
    "])\n",
    "for p in paths_to_check:\n",
    "    print('Exists:', p, p.exists())\n",
    "\n",
    "missing_crit = [p for p in paths_to_check[:2] if not p.exists()]\n",
    "if missing_crit:\n",
    "    raise FileNotFoundError(f\"❌ Không tìm thấy thư mục ảnh train/val sau khi chuẩn hoá: {missing_crit}. Hãy kiểm tra lại dataset4corner cấu trúc hoặc chỉnh FORCE_DATASET_PATH.\")\n",
    "\n",
    "# Chọn checkpoint\n",
    "ckpt_candidates = [\n",
    "    Path('runs/pose_plate_merged/weights/best.pt'),\n",
    "    Path('runs/pose_plate/weights/best.pt'),\n",
    "    Path('best.pt'),\n",
    "]\n",
    "ckpt = next((p for p in ckpt_candidates if p.exists()), None)\n",
    "if ckpt:\n",
    "    print(f'Loading checkpoint: {ckpt}')\n",
    "    model = YOLO(str(ckpt))\n",
    "else:\n",
    "    print('No existing checkpoint found -> init new small pose model')\n",
    "    backbone_opts = ['yolo11n-pose.pt','yolov8n-pose.pt']\n",
    "    backbone = next((b for b in backbone_opts if Path(b).exists()), None)\n",
    "    model = YOLO(backbone) if backbone else YOLO('yolov8n-pose.pt')  # sẽ tải nếu có mạng\n",
    "\n",
    "# Tham số train nhanh để TEST local\n",
    "EPOCHS = int(os.environ.get('TEST_EPOCHS', '5'))  # đổi biến môi trường nếu muốn\n",
    "BATCH  = int(os.environ.get('TEST_BATCH',  '4'))\n",
    "\n",
    "train_args = dict(\n",
    "    data=str(out_yaml),\n",
    "    imgsz=640,\n",
    "    epochs=EPOCHS,\n",
    "    patience=0,              # Không early stop trong test ngắn\n",
    "    batch=BATCH,\n",
    "    device=0 if torch.cuda.is_available() else 'cpu',\n",
    "    workers=2 if IS_KAGGLE else 4,\n",
    "    project='runs',\n",
    "    name='pose_plate_local_test',\n",
    "    exist_ok=True,\n",
    "    pretrained=False,        # tránh tải online\n",
    "    cache='ram',\n",
    "    plots=True,\n",
    "    save_period=0,\n",
    "    seed=42,\n",
    "    amp=False,               # bạn có thể bật True nếu torch + GPU hỗ trợ\n",
    "    verbose=True,\n",
    ")\n",
    "print('Train args:', train_args)\n",
    "\n",
    "results = model.train(**train_args)\n",
    "print('Done. Best metrics path in runs/pose_plate_local_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872c8c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gộp 3 dataset: dataset4corner, archive_1, archive_2 vào merged_dataset\n",
    "import shutil, os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from collections import defaultdict\n",
    "def merge_datasets(sources, dest):\n",
    "    dest = Path(dest)\n",
    "    for split in ['images/train', 'images/val', 'labels/train', 'labels/val']:\n",
    "        (dest/split).mkdir(parents=True, exist_ok=True)\n",
    "    name_map = defaultdict(int)\n",
    "    for src in sources:\n",
    "        src = Path(src)\n",
    "        for split in ['images/train', 'images/val', 'labels/train', 'labels/val']:\n",
    "            src_dir = src/split\n",
    "            if not src_dir.exists(): continue\n",
    "            for f in src_dir.glob('*'):\n",
    "                stem = f.stem\n",
    "                ext = f.suffix\n",
    "                # Đảm bảo không trùng tên file\n",
    "                while True:\n",
    "                    new_name = f\"{stem}{'' if name_map[stem]==0 else f'_{name_map[stem]}'}{ext}\"\n",
    "                    out_path = dest/split/new_name\n",
    "                    if not out_path.exists(): break\n",
    "                    name_map[stem] += 1\n",
    "                shutil.copy2(f, out_path)\n",
    "    print('Đã gộp xong datasets vào', dest)\n",
    "    # Tạo YAML mới\n",
    "    yaml_path = dest/'data_merged.yaml'\n",
    "    yaml_dict = {\n",
    "        'train': str((dest/'images/train').resolve()),\n",
    "        'val': str((dest/'images/val').resolve()),\n",
    "        'nc': 1,\n",
    "        'names': ['plate'],\n",
    "        'kpt_shape': [4,3],\n",
    "        'flip_idx': [[0,1],[2,3]]\n",
    "    }\n",
    "    with open(yaml_path, 'w') as f:\n",
    "        yaml.safe_dump(yaml_dict, f, sort_keys=False)\n",
    "    print('Đã tạo YAML:', yaml_path)\n",
    "merge_datasets(['dataset4corner', 'archive_1', 'archive_2'], 'merged_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e1ce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiền xử lý ảnh: grayscale, CLAHE, gamma, edge map cho merged_dataset\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "def preprocess_img(in_path, out_path, alpha=0.6, gamma=1.2):\n",
    "    img = cv2.imread(str(in_path))\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    cl = clahe.apply(gray)\n",
    "    invG = 1.0 / gamma\n",
    "    lut = np.array([((i / 255.0) ** invG) * 255 for i in np.arange(256)]).astype('uint8')\n",
    "    gam = cv2.LUT(cl, lut)\n",
    "    edges = cv2.Canny(gam, 50, 150)\n",
    "    edges = cv2.dilate(edges, np.ones((3,3), np.uint8))\n",
    "    edges_col = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)\n",
    "    edges_col[:,:,1] = edges\n",
    "    edges_col[:,:,0] = edges\n",
    "    blended = cv2.addWeighted(cv2.cvtColor(gam, cv2.COLOR_GRAY2BGR), 1-alpha, edges_col, alpha, 0)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    cv2.imwrite(str(out_path), blended)\n",
    "def preprocess_dataset(src_root, dst_root):\n",
    "    src_root = Path(src_root)\n",
    "    dst_root = Path(dst_root)\n",
    "    for split in ['train', 'val']:\n",
    "        src_dir = src_root/'images'/split\n",
    "        dst_dir = dst_root/'images'/split\n",
    "        dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "        files = list(src_dir.glob('*'))\n",
    "        for f in tqdm(files, desc=f'Preprocessing {split} images'):\n",
    "            out_f = dst_dir/f.name\n",
    "            preprocess_img(f, out_f)\n",
    "    # Copy labels giữ nguyên\n",
    "    for split in ['train', 'val']:\n",
    "        src_lbl = src_root/'labels'/split\n",
    "        dst_lbl = dst_root/'labels'/split\n",
    "        dst_lbl.mkdir(parents=True, exist_ok=True)\n",
    "        for f in src_lbl.glob('*'):\n",
    "            shutil.copy2(f, dst_lbl/f.name)\n",
    "    print('Tiền xử lý xong, ảnh lưu ở', dst_root)\n",
    "preprocess_dataset('merged_dataset', 'prep_merged_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f492530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiền xử lý blend edge map vào ảnh gốc/xám (giữ thông tin thật, overlay cạnh)\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "def preprocess_img_blend(in_path, out_path, alpha=0.25, gamma=1.2, use_gray=False):\n",
    "    img = cv2.imread(str(in_path))\n",
    "    if use_gray:\n",
    "        base = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        base = cv2.cvtColor(base, cv2.COLOR_GRAY2BGR)\n",
    "    else:\n",
    "        base = img.copy()\n",
    "    # CLAHE + gamma\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    cl = clahe.apply(gray)\n",
    "    invG = 1.0 / gamma\n",
    "    lut = np.array([((i / 255.0) ** invG) * 255 for i in np.arange(256)]).astype('uint8')\n",
    "    gam = cv2.LUT(cl, lut)\n",
    "    # Edge map\n",
    "    edges = cv2.Canny(gam, 50, 150)\n",
    "    edges = cv2.dilate(edges, np.ones((3,3), np.uint8))\n",
    "    edges_col = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)\n",
    "    edges_col[:,:,1] = edges\n",
    "    edges_col[:,:,0] = edges\n",
    "    # Blend edge map lên base (ảnh gốc/xám)\n",
    "    blended = cv2.addWeighted(base, 1-alpha, edges_col, alpha, 0)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    cv2.imwrite(str(out_path), blended)\n",
    "def preprocess_dataset_blend(src_root, dst_root, alpha=0.25, gamma=1.2, use_gray=False):\n",
    "    src_root = Path(src_root)\n",
    "    dst_root = Path(dst_root)\n",
    "    for split in ['train', 'val']:\n",
    "        src_dir = src_root/'images'/split\n",
    "        dst_dir = dst_root/'images'/split\n",
    "        dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "        files = list(src_dir.glob('*'))\n",
    "        for f in tqdm(files, desc=f'Preprocessing {split} images (blend edge)'):\n",
    "            out_f = dst_dir/f.name\n",
    "            preprocess_img_blend(f, out_f, alpha=alpha, gamma=gamma, use_gray=use_gray)\n",
    "    # Copy labels giữ nguyên\n",
    "    for split in ['train', 'val']:\n",
    "        src_lbl = src_root/'labels'/split\n",
    "        dst_lbl = dst_root/'labels'/split\n",
    "        dst_lbl.mkdir(parents=True, exist_ok=True)\n",
    "        for f in src_lbl.glob('*'):\n",
    "            shutil.copy2(f, dst_lbl/f.name)\n",
    "    print('Tiền xử lý xong, ảnh blend cạnh lưu ở', dst_root)\n",
    "# Ví dụ: blend edge map vào ảnh gốc, alpha=0.25 (có thể tăng lên 0.3-0.4 nếu muốn cạnh nổi hơn)\n",
    "preprocess_dataset_blend('merged_dataset', 'prep_merged_dataset_blend', alpha=0.25, gamma=1.2, use_gray=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8642721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiểm tra nhãn: overlay keypoint từ label lên ảnh, lưu ra check_labels_vis\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import random\n",
    "def draw_kpts(img, kpts, color=(0,255,255)):\n",
    "    for i, (x, y, v) in enumerate(kpts):\n",
    "        if v > 0:\n",
    "            cv2.circle(img, (int(x), int(y)), 5, color, -1)\n",
    "            cv2.putText(img, str(i), (int(x)+2, int(y)-2), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1, cv2.LINE_AA)\n",
    "    return img\n",
    "def check_labels_vis(img_dir, label_dir, out_dir, n_samples=30, img_size=640):\n",
    "    img_dir = Path(img_dir)\n",
    "    label_dir = Path(label_dir)\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    img_files = list(img_dir.glob('*.jpg')) + list(img_dir.glob('*.png'))\n",
    "    random.shuffle(img_files)\n",
    "    for img_path in img_files[:n_samples]:\n",
    "        label_path = label_dir / (img_path.stem + '.txt')\n",
    "        if not label_path.exists(): continue\n",
    "        img = cv2.imread(str(img_path))\n",
    "        h, w = img.shape[:2]\n",
    "        with open(label_path) as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) < 5+12: continue  # 4 kpts * 3 + 5 bbox fields\n",
    "                kpts = []\n",
    "                for i in range(4):\n",
    "                    x = float(parts[5 + i*3]) * w\n",
    "                    y = float(parts[5 + i*3 + 1]) * h\n",
    "                    v = float(parts[5 + i*3 + 2])\n",
    "                    kpts.append((x, y, v))\n",
    "                img = draw_kpts(img, kpts)\n",
    "        cv2.imwrite(str(out_dir / img_path.name), img)\n",
    "    print(f'Đã lưu {n_samples} ảnh kiểm tra nhãn vào {out_dir}')\n",
    "# Ví dụ sử dụng: kiểm tra nhãn trên bộ blend\n",
    "check_labels_vis('prep_merged_dataset_blend/images/val', 'prep_merged_dataset_blend/labels/val', 'check_labels_vis', n_samples=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20789d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xuất file txt dự đoán YOLOv8-pose ra pred_labels và kiểm tra tự động keypoint giữa label và dự đoán (KHÔNG dùng hàm, chỉ chạy tuần tự các bước, mọi thứ trong 1 cell)\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# --- BƯỚC 1: Xuất file txt dự đoán YOLOv8-pose ra pred_labels ---\n",
    "model_path = 'best.pt'  # Đổi tên file model nếu cần\n",
    "img_dir = 'prep_merged_dataset_blend/images/val'\n",
    "out_dir = 'pred_labels'\n",
    "conf = 0.25\n",
    "device = 0\n",
    "\n",
    "Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "model = YOLO(model_path)\n",
    "img_dir_p = Path(img_dir)\n",
    "img_files = list(img_dir_p.glob('*.jpg')) + list(img_dir_p.glob('*.png'))\n",
    "for img_path in img_files:\n",
    "    results = model(img_path, conf=conf, device=device)\n",
    "    for r in results:\n",
    "        if hasattr(r, 'keypoints') and r.keypoints is not None and r.boxes is not None:\n",
    "            for box, kpts, score in zip(r.boxes.xywhn.cpu().numpy(), r.keypoints.xyn.cpu().numpy(), r.boxes.conf.cpu().numpy()):\n",
    "                line = [0]  # class 0\n",
    "                line += list(box)\n",
    "                for kp in kpts:\n",
    "                    line += list(kp)\n",
    "                txt_path = Path(out_dir) / (img_path.stem + '.txt')\n",
    "                with open(txt_path, 'a') as f:\n",
    "                    f.write(' '.join(f'{x:.6f}' for x in line) + '\\n')\n",
    "print(f'Đã xuất file txt dự đoán vào {Path(out_dir).resolve()}')\n",
    "\n",
    "# --- BƯỚC 2: Kiểm tra tự động keypoint giữa label và dự đoán ---\n",
    "label_dir = 'prep_merged_dataset_blend/labels/val'\n",
    "pred_dir = out_dir\n",
    "out_dir_check = 'check_kpt_error'\n",
    "error_thresh = 10\n",
    "\n",
    "Path(out_dir_check).mkdir(parents=True, exist_ok=True)\n",
    "img_files = list(img_dir_p.glob('*.jpg')) + list(img_dir_p.glob('*.png'))\n",
    "n_checked = 0\n",
    "n_error = 0\n",
    "for img_path in img_files:\n",
    "    label_path = Path(label_dir) / (img_path.stem + '.txt')\n",
    "    pred_path = Path(pred_dir) / (img_path.stem + '.txt')\n",
    "    if not label_path.exists() or not pred_path.exists(): continue\n",
    "    img = cv2.imread(str(img_path))\n",
    "    h, w = img.shape[:2]\n",
    "    # Parse label kpts\n",
    "    kpts_gt = []\n",
    "    with open(label_path) as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 5+12: continue\n",
    "            for i in range(4):\n",
    "                x = float(parts[5 + i*3]) * w\n",
    "                y = float(parts[5 + i*3 + 1]) * h\n",
    "                v = float(parts[5 + i*3 + 2])\n",
    "                kpts_gt.append((x, y, v))\n",
    "    # Parse pred kpts\n",
    "    kpts_pred = []\n",
    "    with open(pred_path) as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 5+12: continue\n",
    "            for i in range(4):\n",
    "                x = float(parts[5 + i*3]) * w\n",
    "                y = float(parts[5 + i*3 + 1]) * h\n",
    "                v = float(parts[5 + i*3 + 2])\n",
    "                kpts_pred.append((x, y, v))\n",
    "    # Tính sai số\n",
    "    if len(kpts_gt) != 4 or len(kpts_pred) != 4:\n",
    "        continue\n",
    "    errors = [np.linalg.norm(np.array(kpts_gt[i][:2]) - np.array(kpts_pred[i][:2])) for i in range(4)]\n",
    "    n_checked += 1\n",
    "    if any(e > error_thresh for e in errors):\n",
    "        n_error += 1\n",
    "        # Vẽ GT: xanh, Pred: đỏ\n",
    "        img_vis = img.copy()\n",
    "        for i, (x, y, v) in enumerate(kpts_gt):\n",
    "            if v > 0:\n",
    "                cv2.circle(img_vis, (int(x), int(y)), 5, (0,255,0), -1)\n",
    "                cv2.putText(img_vis, f'GT{i}', (int(x)+2, int(y)-2), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 1, cv2.LINE_AA)\n",
    "        for i, (x, y, v) in enumerate(kpts_pred):\n",
    "            if v > 0:\n",
    "                cv2.circle(img_vis, (int(x), int(y)), 5, (0,0,255), -1)\n",
    "                cv2.putText(img_vis, f'P{i}', (int(x)+2, int(y)+12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1, cv2.LINE_AA)\n",
    "        cv2.imwrite(str(Path(out_dir_check) / img_path.name), img_vis)\n",
    "print(f'Đã kiểm tra {n_checked} ảnh, phát hiện {n_error} ảnh có keypoint lệch > {error_thresh} px. Ảnh lỗi lưu ở {out_dir_check}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47c60d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã kiểm tra 0 ảnh, phát hiện 0 ảnh có keypoint lệch > 10 px. Ảnh lỗi lưu ở check_kpt_error\n"
     ]
    }
   ],
   "source": [
    "# Kiểm tra sai số keypoint giữa label thật và dự đoán (pred_labels), lưu ảnh lỗi vào check_kpt_error\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "img_dir = 'prep_merged_dataset_blend/images/val'\n",
    "label_dir = 'prep_merged_dataset_blend/labels/val'\n",
    "pred_dir = 'pred_labels'\n",
    "out_dir_check = 'check_kpt_error'\n",
    "error_thresh = 10\n",
    "\n",
    "Path(out_dir_check).mkdir(parents=True, exist_ok=True)\n",
    "img_dir_p = Path(img_dir)\n",
    "img_files = list(img_dir_p.glob('*.jpg')) + list(img_dir_p.glob('*.png'))\n",
    "n_checked = 0\n",
    "n_error = 0\n",
    "for img_path in img_files:\n",
    "    label_path = Path(label_dir) / (img_path.stem + '.txt')\n",
    "    pred_path = Path(pred_dir) / (img_path.stem + '.txt')\n",
    "    if not label_path.exists() or not pred_path.exists():\n",
    "        continue\n",
    "    img = cv2.imread(str(img_path))\n",
    "    h, w = img.shape[:2]\n",
    "    # Parse label kpts\n",
    "    kpts_gt = []\n",
    "    with open(label_path) as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 5+12: continue\n",
    "            for i in range(4):\n",
    "                x = float(parts[5 + i*3]) * w\n",
    "                y = float(parts[5 + i*3 + 1]) * h\n",
    "                v = float(parts[5 + i*3 + 2])\n",
    "                kpts_gt.append((x, y, v))\n",
    "    # Parse pred kpts\n",
    "    kpts_pred = []\n",
    "    with open(pred_path) as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 5+12: continue\n",
    "            for i in range(4):\n",
    "                x = float(parts[5 + i*3]) * w\n",
    "                y = float(parts[5 + i*3 + 1]) * h\n",
    "                v = float(parts[5 + i*3 + 2])\n",
    "                kpts_pred.append((x, y, v))\n",
    "    # Tính sai số\n",
    "    if len(kpts_gt) != 4 or len(kpts_pred) != 4:\n",
    "        continue\n",
    "    errors = [np.linalg.norm(np.array(kpts_gt[i][:2]) - np.array(kpts_pred[i][:2])) for i in range(4)]\n",
    "    n_checked += 1\n",
    "    if any(e > error_thresh for e in errors):\n",
    "        n_error += 1\n",
    "        # Vẽ GT: xanh, Pred: đỏ\n",
    "        img_vis = img.copy()\n",
    "        for i, (x, y, v) in enumerate(kpts_gt):\n",
    "            if v > 0:\n",
    "                cv2.circle(img_vis, (int(x), int(y)), 5, (0,255,0), -1)\n",
    "                cv2.putText(img_vis, f'GT{i}', (int(x)+2, int(y)-2), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 1, cv2.LINE_AA)\n",
    "        for i, (x, y, v) in enumerate(kpts_pred):\n",
    "            if v > 0:\n",
    "                cv2.circle(img_vis, (int(x), int(y)), 5, (0,0,255), -1)\n",
    "                cv2.putText(img_vis, f'P{i}', (int(x)+2, int(y)+12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1, cv2.LINE_AA)\n",
    "        cv2.imwrite(str(Path(out_dir_check) / img_path.name), img_vis)\n",
    "print(f'Đã kiểm tra {n_checked} ảnh, phát hiện {n_error} ảnh có keypoint lệch > {error_thresh} px. Ảnh lỗi lưu ở {out_dir_check}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
